{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM8kLxUEVc3Z"
   },
   "source": [
    "# Natural Language Processing Demystified | Preprocessing\n",
    "## Sources\n",
    "Largely based on\n",
    "https://nlpdemystified.org<br>\n",
    "https://github.com/nitinpunjabi/nlp-demystified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Installation and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btimL_w92Q3P"
   },
   "source": [
    "### spaCy upgrade and package installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7Ll-fUK2VZs"
   },
   "source": [
    "At the time this notebook was created, spaCy had newer releases but Colab was still using version 2.x by default. So the first step is to upgrade spaCy.\n",
    "<br><br>\n",
    "**IMPORTANT**<br>\n",
    "If you're running this in the cloud rather than using a local Jupyter server on your machine, then the notebook will **timeout** after a period of inactivity. If that happens and you don't reconnect in time, you will need to upgrade spaCy again and reinstall the requisite statistical packages.\n",
    "<br><br>\n",
    "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
    "https://research.google.com/colaboratory/local-runtimes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cve1-G7j2VTN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (3.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (1.22.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (49.2.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (1.9.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (8.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from packaging>=20.0->spacy) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.0.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from jinja2->spacy) (2.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "z-FDdbc62VHd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    3.4.1                         \n",
      "Location         /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages/spacy\n",
      "Platform         macOS-12.5.1-arm64-arm-64bit  \n",
      "Python version   3.8.9                         \n",
      "Pipelines        en_core_web_sm (3.4.0)        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8vW9svTE289D"
   },
   "outputs": [],
   "source": [
    " import spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfJKSJEU2U_s"
   },
   "source": [
    "After importing spaCy, the next thing we need to do is load a suitable statistical model for our project. spaCy offers a variety of models for different languages. These models help with tokenization, part-of-speech tagging, named entity recognition, and more.\n",
    "\n",
    "Here, we're loading the **en_core_web_sm** model which is the smallest English model spaCy offers and is a good starting point for NLP tasks.<br>\n",
    "https://spacy.io/models/en#en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v6TGQff2iu6"
   },
   "source": [
    "Since we upgraded spaCy, we'll need to download the statistical model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4uOyHDNb2i5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (49.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.27.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.22.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.62.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.11)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jospolfliet/Library/Python/3.8/lib/python/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mWDrpxDk2_r2"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7YCbWtG3LJO"
   },
   "source": [
    "**en_core_web_sm** is trained on OntoNotes 5 which is an annotated corpus comprising news, blogs, transcripts, etc. Put simply, this means a bunch of documents were labelled with information such as how each sentence should be parsed, whether a particular word is a noun or adjective or other part-of-speech, whether a word is a special entity like a person or a real-world organization, and other language-related labels. A statistical model was then generated from these labelled documents.<br>\n",
    "https://catalog.ldc.upenn.edu/LDC2013T19\n",
    "<br><br>\n",
    "You can learn more about the available spaCy models at these links:<br>\n",
    "https://spacy.io/models<br>\n",
    "https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvF_udvi3OTO"
   },
   "source": [
    "After loading the model, the _nlp_ variable now references a **Language** class instance which contains language-specific rules for various tasks (e.g. tokenization) and a processing pipeline.<br>\n",
    "https://spacy.io/api/language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DAYGtQpT3UNN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unmnGRu8D-wa"
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "Course module for this demo:\n",
    "https://www.nlpdemystified.org/course/tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just so you can start here and skip the installation part when you did that before.\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLUcGm3IbQki"
   },
   "source": [
    "### Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13twUCp2i_p8"
   },
   "source": [
    "We pass whatever text we want to process to _nlp_, which returns a **Doc** container object containing the tokenized text and a number of annotations for each token. These annotations are discussed in follow-up videos. You can learn more about the **Doc** object here:<br>\n",
    "https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "BIoEJZ-IkHQ4"
   },
   "outputs": [],
   "source": [
    "# Sample sentence.\n",
    "s = \"It's not about the money (only $20.15), it's about sending a message :). üöÄüíéüôå\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMWZK3ZSk9-f"
   },
   "source": [
    "We can iterate over this **Doc** object and view the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "8SzqhZuulAe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'not', 'about', 'the', 'money', '(', 'only', '$', '20.15', ')', ',', 'it', \"'s\", 'about', 'sending', 'a', 'message', ':)', '.', 'üöÄ', 'üíé', 'üôå']\n"
     ]
    }
   ],
   "source": [
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai1obkB93GdD"
   },
   "source": [
    "Note how\n",
    "- `it's` is separated in `it` and `'s`\n",
    "- the currency symbol and amount are separated.\n",
    "- punctuation like `.` is separated when it has a function at the end of the sentence\n",
    "- punctuation like `.` is not separated when it is an indivisible part of a token\n",
    "- the period at the end of the sentence is its own token\n",
    "- emoji's are one token each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWH49gIh3hqN"
   },
   "source": [
    "The **Doc** object can be indexed and sliced like a regular list. The **Doc** object contains **Token** and **Span** objects, which offer different views into the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MwLrxRsE3oKI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.15\n"
     ]
    }
   ],
   "source": [
    "# We can view an individual token by indexing into the Doc object.\n",
    "print(doc[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bGapNHYQFYVa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# A Doc object is a container of other objects, namely Token and Span objects.\n",
    "print(type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "EtL2IgIAGOd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's not\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "# Slicing a Doc object returns a Span object.\n",
    "print(doc[0:3])\n",
    "print(type(doc[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "xybH4jjYGo73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It', 0), (\"'s\", 1), ('not', 2), ('about', 3), ('the', 4), ('money', 5), ('(', 6), ('only', 7), ('$', 8), ('20.15', 9), (')', 10), (',', 11), ('it', 12), (\"'s\", 13), ('about', 14), ('sending', 15), ('a', 16), ('message', 17), ('.', 18), ('üöÄ', 19), ('üíé', 20), ('üôå', 21)]\n"
     ]
    }
   ],
   "source": [
    "# Access a token's index in a sentence.\n",
    "print([(t.text, t.i) for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TqE980F4Vrt"
   },
   "source": [
    "Spacy's tokenization is _non-destructive_, which means the original input can be reconstructed from the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "OjXb8mR_DK-1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's not about the money (only $20.15), it's about sending a message. üöÄüíéüôå\n"
     ]
    }
   ],
   "source": [
    "# You can view the original input like so:\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokens have many useful properties\n",
    "print(doc[10])\n",
    "doc[10].is_punct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[sending, a, message]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(doc[15])\n",
    "list(doc[15].subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73vuSX7MDK79"
   },
   "source": [
    "You can learn more about the **Token** and **Span** objects here:<br>\n",
    "https://spacy.io/api/token<br>\n",
    "https://spacy.io/api/span\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lume_1UP6ySQ"
   },
   "source": [
    "We can also tokenize multiple sentences and access each sentence individually using the **Doc** object's _sents_ property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "mPZ86x0hDK4m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### JUST PUT IN ANOTHER 30K IN NOK CALLS LET'S GO!\n",
      "##### $GME $NOK BUY AND HOLD üöÄüöÄ üöÄüöÄ.\n",
      "##### We need to stick together and üíéüñê the ever lovin shit out of this opportunity.\n",
      "##### We will leave no man or woman behind!\n",
      "##### Forward!\n",
      "##### Together!\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"JUST PUT IN ANOTHER 30K IN NOK CALLS LET'S GO! $GME $NOK BUY AND HOLD üöÄüöÄ üöÄüöÄ. We need to stick together and üíéüñê the ever lovin shit out of this opportunity. We will leave no man or woman behind! Forward! Together!\"\"\"\n",
    "\n",
    "doc = nlp(s)\n",
    "\n",
    "# Look at individual sentences (there should be multiple 'Span' objects).\n",
    "for sent in doc.sents:\n",
    "    print(f\"##### {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvSfDUyK06Qg"
   },
   "source": [
    "### Tokenization Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "fyywcBrCHzSk"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE:\n",
    "# 1) Tokenize the following text\n",
    "# 2) Iterate through the tokens to check whether there's a currency symbol.\n",
    "# 3) If there is, and the currency label is followed by a number, print\n",
    "#    both the symbol and the number.\n",
    "# \n",
    "# Look through https://spacy.io/api/token#attributes on how to check whether\n",
    "# a token is a currency symbol or a number.\n",
    "#\n",
    "# Expected output: \"$20\".\n",
    "s = \"It's not about the money (only $20.15), it's about sending a message. üöÄüíéüôå\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: find the longest token in the WallStreetBets dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "skajI-OZDK0t"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Learn how the spaCy tokenizer works and how to customize it:\n",
    "# https://spacy.io/usage/linguistic-features#tokenization\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ikbnyb8rDKv9"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Read through spaCy-101 and if you're interested, check out their course\n",
    "# on spaCy itself (link on the page).\n",
    "# https://spacy.io/usage/spacy-101\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMArLP91DKUW"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Look up how to tokenize the sentence below using NLTK. The imports \n",
    "# are done for you. Does the NLTK tokenizer handle \"N.Y.C.\" correctly?\n",
    "#\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "s = \"Let's go to N.Y.C. for the weekend.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMbm9tTakDdy"
   },
   "source": [
    "**NOTE**: Different tokenizers will give subtly different results based on the rules they use. Experiment with different tokenizers and use the one best suited for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUsfYCpVT4nI"
   },
   "source": [
    "# Basic Preprocessing\n",
    "## Case-Folding, Stop Word Removal, Stemming, and Lemmatization.\n",
    "\n",
    "Course module for this demo:\n",
    "https://www.nlpdemystified.org/course/basic-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mg6dga4JePf2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgDDrCeI8f-4"
   },
   "source": [
    "spaCy performs all these preprocessing steps (except stemming) behind the scenes for you. Inline with its non-destructive policy, the tokens aren't modified directly. Rather, each **Token** object has a number of attributes which can help you get views of your document with these pre-processing steps applied. The attributes a **Token** has can be found here:<br>\n",
    "https://spacy.io/api/token#attributes\n",
    "<br><br>\n",
    "More information about spaCy's processing pipeline:<br>\n",
    "https://spacy.io/usage/processing-pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "jDEMR6En1j3H"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "s = \"Once you're done with GME - $AG and $SLV, the gentleman's short squeeze, driven by macro fundamentals :/\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwA1ct0obYlR"
   },
   "source": [
    "### Case-Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biBPWrVd9BrK"
   },
   "source": [
    "View your document with case-folding using the *lower_* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "1nt4RpzdgQQL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['once', 'you', \"'re\", 'done', 'with', 'gme', '-', '$', 'ag', 'and', '$', 'slv', ',', 'the', 'gentleman', \"'s\", 'short', 'squeeze', ',', 'driven', 'by', 'macro', 'fundamentals', ':/']\n"
     ]
    }
   ],
   "source": [
    "print([t.lower_ for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HL46I4sH9OMq"
   },
   "source": [
    "You can also apply conditions when generating these views. For example, we can skip case-folding if a token is the start of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "IO0PQ8IFhOlZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Once, 'you', \"'re\", 'done', 'with', 'gme', '-', '$', 'ag', 'and', '$', 'slv', ',', 'the', 'gentleman', \"'s\", 'short', 'squeeze', ',', 'driven', 'by', 'macro', 'fundamentals', ':/']\n"
     ]
    }
   ],
   "source": [
    "print([t.lower_ if not t.is_sent_start else t for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7pTz8XJbmaT"
   },
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZLqqmHa9cRx"
   },
   "source": [
    "spaCy comes with a default stop word list. To view your document with stop words removed, you can use the *is_stop* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "9kvXbuDEhOxu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'his', 'becomes', 'whereafter', 'few', 'we', 'anyway', 'one', 'within', 'or', 'something', 'too', 'these', 'fifty', 'which', 'front', 'almost', 'me', 'from', 'five', '‚Äòm', 'get', 'then', 'my', 'they', 'on', 'unless', 'everyone', 'keep', 'around', 'to', 'onto', 'via', 'anywhere', 'hence', 'yourself', 'two', 'since', 'even', '‚Äòve', 'always', 'and', 'former', 'four', 'under', '‚Äòre', 'all', 'been', 'forty', 'will', 'though', 'using', 'besides', 'except', 'put', 'noone', '‚Äôll', 'has', 'namely', 'during', 'serious', 'the', 'beside', 'hereafter', 'in', 'back', 'another', 'where', 'here', 'thru', 'whom', 'than', 'until', 'do', '‚Äòd', 'sometimes', 'thereupon', 'take', 'else', 'must', 'but', 'are', 'various', 'top', 'n‚Äôt', 'am', 'make', 'once', 'cannot', 'him', 'be', 'other', 'hers', \"'d\", 'without', 'please', 'already', \"'s\", '‚Äôre', 'you', 'those', 'three', 'amongst', 'thereby', 'therein', 'into', 'formerly', 'who', 'whereby', 'off', 'bottom', 'as', 'made', 'would', 'across', 'themselves', 'latter', 'meanwhile', 'same', 'nowhere', 'by', 'many', 'nobody', 'everywhere', 'out', 'afterwards', 'amount', 'i', 'used', 're', 'twelve', 'your', 'each', 'sometime', 'it', \"n't\", 'seeming', \"'re\", 'move', 'nor', 'also', 'does', 'should', 'really', 'eight', 'while', 'less', 'much', 'whence', 'mine', 'between', 'nothing', 'ours', 'indeed', 'seemed', 'say', 'latterly', 'hereby', 'ca', 'rather', '‚Äòs', 'perhaps', 'anyhow', 'yours', 'them', 'seem', 'before', 'done', 'upon', 'every', 'never', \"'ll\", 'anyone', 'sixty', 'that', 'if', 'such', \"'m\", 'with', 'therefore', 'thence', 'full', 'being', 'together', 'third', 'several', 'how', 'had', 'six', 'only', 'due', 'whereupon', 'any', 'name', 'no', 'least', 'nine', 'first', 'about', 'very', 'show', 'whoever', 'is', \"'ve\", 'next', '‚Äôve', 'a', 'mostly', 'what', 'whole', 'us', 'someone', 'most', 'ever', 'just', '‚Äôm', '‚Äôs', 'somewhere', 'against', '‚Äôd', 'quite', 'whereas', 'may', 'herself', 'thereafter', 'moreover', 'could', 'can', 'whither', 'up', 'none', 'side', 'regarding', 'her', 'towards', 'empty', 'either', 'myself', 'well', 'itself', 'last', 'call', 'through', 'above', 'ourselves', 'not', 'own', 'yourselves', 'among', 'more', 'see', 'when', 'yet', 'now', 'everything', 'have', 'there', 'elsewhere', 'he', 'neither', 'some', 'throughout', 'for', 'beyond', 'thus', 'hundred', 'below', 'still', 'our', 'down', 'were', 'seems', 'both', 'alone', 'might', 'behind', 'others', 'whatever', 'enough', 'whose', 'over', 'eleven', 'ten', 'toward', 'himself', 'an', 'fifteen', 'wherever', 'was', 'their', 'doing', 'further', 'become', 'twenty', 'beforehand', 'of', 'part', 'somehow', 'hereupon', 'whenever', 'along', 'otherwise', '‚Äòll', 'again', 'why', 'wherein', 'go', 'becoming', 'often', 'because', 'although', 'whether', 'herein', 'give', 'after', 'however', 'nevertheless', 'so', 'became', 'at', 'anything', 'this', 'she', 'did', 'n‚Äòt', 'its', 'per'}\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "# spaCy's default stop word list.\n",
    "print(nlp.Defaults.stop_words)\n",
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "oAS1xmgOhO5y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GME, -, $, AG, $, SLV, ,, gentleman, short, squeeze, ,, driven, macro, fundamentals, :/]\n"
     ]
    }
   ],
   "source": [
    "print([t for t in doc if not t.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPd1aiLrbqcK"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKidP32Y_qcE"
   },
   "source": [
    "It's similar with lemmatization. You can view your document with lemmatization applied through the *lemma_* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "fhdRleESkzTu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Once', 'once'),\n",
       " ('you', 'you'),\n",
       " (\"'re\", 'be'),\n",
       " ('done', 'do'),\n",
       " ('with', 'with'),\n",
       " ('GME', 'GME'),\n",
       " ('-', '-'),\n",
       " ('$', '$'),\n",
       " ('AG', 'AG'),\n",
       " ('and', 'and'),\n",
       " ('$', '$'),\n",
       " ('SLV', 'SLV'),\n",
       " (',', ','),\n",
       " ('the', 'the'),\n",
       " ('gentleman', 'gentleman'),\n",
       " (\"'s\", \"'s\"),\n",
       " ('short', 'short'),\n",
       " ('squeeze', 'squeeze'),\n",
       " (',', ','),\n",
       " ('driven', 'drive'),\n",
       " ('by', 'by'),\n",
       " ('macro', 'macro'),\n",
       " ('fundamentals', 'fundamental'),\n",
       " (':/', ':/')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.text, t.lemma_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question:* Why would we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOXJI061npqN"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Find out how to add and remove your own stop words in spaCy. Add the \n",
    "# word 'told' as a stop word, test that it works, then remove it from \n",
    "# the stop word list.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLcCYIy-lP1u"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Read up on how to add your own custom attributes to Token objects\n",
    "# and try adding one of your own.\n",
    "# https://spacy.io/usage/processing-pipelines#custom-components-attributes\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9HLYYUt1kOP"
   },
   "source": [
    "## Named Entity Recognition, and Parsing\n",
    "\n",
    "Course module for this demo:\n",
    "https://www.nlpdemystified.org/course/advanced-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr5SqjHwSWpI"
   },
   "source": [
    "spaCy performs Part-of-Speech (POS) tagging, Named Entity Recognition (NER), and parsing as part of its default pipeline in the *nlp* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "shgWRMCq1kmy"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "s = \"Once you're done with GME, $AG and $SLV, the gentleman's short squeeze, driven by macro fundamentals.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jte6K6HJb750"
   },
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2J2BjPyqWFEf"
   },
   "source": [
    "There are multiple ways to access named entities. One way is through the *ent_type_* attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "dWjNrX6koNVj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Once', ''),\n",
       " ('you', ''),\n",
       " (\"'re\", ''),\n",
       " ('done', ''),\n",
       " ('with', ''),\n",
       " ('GME', 'ORG'),\n",
       " (',', ''),\n",
       " ('$', ''),\n",
       " ('AG', 'ORG'),\n",
       " ('and', ''),\n",
       " ('$', ''),\n",
       " ('SLV', 'ORG'),\n",
       " (',', ''),\n",
       " ('the', ''),\n",
       " ('gentleman', ''),\n",
       " (\"'s\", ''),\n",
       " ('short', ''),\n",
       " ('squeeze', ''),\n",
       " (',', ''),\n",
       " ('driven', ''),\n",
       " ('by', ''),\n",
       " ('macro', ''),\n",
       " ('fundamentals', ''),\n",
       " ('.', '')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[(t.text, t.ent_type_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7p8IcNGpBTP"
   },
   "source": [
    "You can also check if a token is an entity before printing it by checking whether the _ent_type_ (note the lack of trailing underscore) attribute is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "0aBng8zdvjly"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('GME', 'ORG'), ('AG', 'ORG'), ('SLV', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "print([(t.text, t.ent_type_) for t in doc if t.ent_type != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lNS65_2XJIY"
   },
   "source": [
    "Another way is through the _ents_ property of the **Doc** object. Here, we iterate through _ents_ and print the entity itself and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "kSCzxs02vjdL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('GME', 'ORG'), ('AG', 'ORG'), ('SLV', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHfZmta8XX9Y"
   },
   "source": [
    "Note how \"next fall\" is outputted above as a single span when you use _ents_.\n",
    "<br><br>\n",
    "You can also access the positions of entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "mSzwRD0MvjTN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('GME', 'ORG', 22, 25), ('AG', 'ORG', 28, 30), ('SLV', 'ORG', 36, 39)]\n"
     ]
    }
   ],
   "source": [
    "print([(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvvQ9_7FdEHT"
   },
   "source": [
    "spaCy is bundled with visualizers for both parsing and named entities.<br>\n",
    "https://spacy.io/usage/visualizers\n",
    "<br><br>\n",
    "Here, we visualize the entities in our sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "87eLywmVZCdw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Once you're done with \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GME\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", $\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AG\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and $\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SLV\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", the gentleman's short squeeze, driven by macro fundamentals.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# We need to set the 'jupyter' variable to True in order to output\n",
    "# the visualization directly. Otherwise, you'll get raw HTML.\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFXPL37Rg2xm"
   },
   "source": [
    "### Fine-tuning models\n",
    "You can fine-tune any NER model too in spaCy. Read the docs if you need that!\n",
    "### Using spaCy's Matcher to find patterns\n",
    "spaCy comes with a host of pattern-matching functionality. Beyond regex, spaCy can match on a variety of attributes such as POS tags, entity labels, lemmas, dependencies, entire phrases, and a lot more. You can learn more here:<br>\n",
    "https://spacy.io/usage/rule-based-matching<br>\n",
    "https://explosion.ai/demos/matcher\n",
    "<br><br>\n",
    "Here, we try to search for patterns that may be useful for our r/WallStreetBets analyser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nhN7p9G8taJ"
   },
   "source": [
    "# Additional Reading and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM4G2KWa8wXO"
   },
   "source": [
    "- https://spacy.io/usage/processing-pipelines\n",
    "- Take the free and succinct spaCy course (available in multiple languages):<br>\n",
    "https://course.spacy.io/\n",
    "- https://spacy.io/usage/spacy-101\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "nlpdemystified-preprocessing.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
