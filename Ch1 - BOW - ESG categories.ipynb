{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5iPecqWFqGa"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "U4KpuOwlGkBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vW9svTE289D"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "from spacy.vectors import Vectors\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1qmVm97FqGc"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://media.githubusercontent.com/media/JosPolfliet/vlerick-mai-nlp-2023/main/DATA/esg_reports.csv\")\n",
        "df[\"subject\"] = df[\"subject\"].fillna(\"Other\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "HHDlY8EIGJ65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iwmp457FqGc"
      },
      "source": [
        "## Get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fufy_rezFqGc"
      },
      "outputs": [],
      "source": [
        "for i in range(10,15):\n",
        "    print(df.iloc[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhPMFwkCFqGd"
      },
      "source": [
        "We'll use the following example sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-8g_XUaFqGd"
      },
      "outputs": [],
      "source": [
        "sentence = df.iloc[67]\n",
        "sentence.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85DhlRx3FqGd"
      },
      "source": [
        "## Clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Osx-rYCNFqGd"
      },
      "outputs": [],
      "source": [
        "punctuations = string.punctuation\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = nlp(sentence[\"text\"].lower())\n",
        "\n",
        "    # Remove OOV words\n",
        "    mytokens = [ word for word in mytokens if not word.is_oov ]\n",
        "\n",
        "    # Lemmatise + lower case\n",
        "    mytokens = [ word.lemma_.strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Remove stop words\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    return mytokens\n",
        "\n",
        "spacy_tokenizer(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWxPPWnwFqGd"
      },
      "source": [
        "## Calculate word frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVT-2h86FqGe"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "tokenFreq = Counter()\n",
        "\n",
        "def countWordFrequencies(example):\n",
        "    tokens = spacy_tokenizer(example)\n",
        "    tokenFreq.update(tokens)\n",
        "\n",
        "df.progress_apply(countWordFrequencies, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3rYvurNFqGe"
      },
      "outputs": [],
      "source": [
        "tokenFreq.total()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVH6d5qgFqGe"
      },
      "outputs": [],
      "source": [
        "len(tokenFreq.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RFdZ_ugFqGe"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuK6DtSrFqGe"
      },
      "outputs": [],
      "source": [
        "print(\"Most frequent\")\n",
        "print(tokenFreq.most_common(10))\n",
        "print(\"Least frequent\")\n",
        "print(tokenFreq.most_common(VOCAB_SIZE)[-10:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SqdbXlYFqGe"
      },
      "outputs": [],
      "source": [
        "tokenMap = {k: i+1 for i, (k, _) in enumerate(tokenFreq.most_common(VOCAB_SIZE-1))}\n",
        "tokenMap[\"<oov>\"]=0\n",
        "tokenMapInverse = {v: k for k,v in tokenMap.items()}\n",
        "print(list(tokenMap.items())[0:10])\n",
        "print([tokenMapInverse[i] for i in range(10)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZaaB9ssFqGf"
      },
      "source": [
        "## Simple model - BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pb_s8Q_FqGf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoU41ISFFqGf"
      },
      "outputs": [],
      "source": [
        "def embed_sentence_BOW(example):\n",
        "    \"\"\"\n",
        "    Simple BOW\n",
        "    \"\"\"\n",
        "    embedding = np.zeros(VOCAB_SIZE)\n",
        "    tokens = spacy_tokenizer(example)\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            tokenIndex = tokenMap[token]\n",
        "        except KeyError: # Out of vocabulary\n",
        "            tokenIndex = 0\n",
        "        embedding[tokenIndex] = embedding[tokenIndex] + 1\n",
        "    return embedding\n",
        "\n",
        "sentence = df.iloc[302]\n",
        "sentence_embedding = embed_sentence_BOW(sentence)\n",
        "\n",
        "print(sentence_embedding)\n",
        "print(sentence_embedding.shape)\n",
        "print(sentence)\n",
        "print(\"What this means: \")\n",
        "\n",
        "for i in range(VOCAB_SIZE):\n",
        "    if sentence_embedding[i]:\n",
        "        print(f\"Token {i} '{tokenMapInverse[i]}' occurs {sentence_embedding[i]} time in sentence.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cQLF8SbFqGf"
      },
      "outputs": [],
      "source": [
        "df[\"embedding\"] = df.progress_apply(embed_sentence_BOW, axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0UhlqFXFqGf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hPl_UUaFqGf"
      },
      "source": [
        "## Classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByNqWPoNFqGf"
      },
      "outputs": [],
      "source": [
        "X = np.stack(df[\"embedding\"])\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGDEwmSiFqGf"
      },
      "outputs": [],
      "source": [
        "X[302:303,]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV-98p6_FqGf"
      },
      "outputs": [],
      "source": [
        "y = np.array(df[\"subject\"])\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1TUcHn1FqGf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4aC67giFqGg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvGxXVwhFqGg"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22141)\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjPUxVzoFqGg"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "clf = RandomForestClassifier(max_depth=2, random_state=22141, class_weight=\"balanced\", n_estimators=300)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhE9VoKTFqGg"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IXH1un-FqGg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (ConfusionMatrixDisplay, classification_report,\n",
        "                             confusion_matrix)\n",
        "from matplotlib import pyplot as plt\n",
        "import datetime\n",
        "from tabulate import tabulate\n",
        "\n",
        "def log_experiment_results(experiment_name, stats, filename=\"experiment_log.md\"):\n",
        "    \"\"\"\n",
        "    Appends experiment results and statistics to a markdown log file.\n",
        "\n",
        "    Parameters:\n",
        "    - experiment_name: str, the name of the experiment\n",
        "    - stats: dict, a dictionary containing the statistics to log\n",
        "    - filename: str, the path to the log file\n",
        "    \"\"\"\n",
        "    stats[\"timestamp\"] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')\n",
        "    stats[\"Experiment Name\"] = experiment_name\n",
        "    try:\n",
        "\n",
        "        df = pd.read_table(filename, sep=\"|\", skipinitialspace=True).drop(0)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "    except (FileNotFoundError, pd.errors.EmptyDataError, pd.errors.ParserError):\n",
        "        df = pd.DataFrame(columns=list(stats.keys()))\n",
        "\n",
        "    df = pd.concat([df, pd.DataFrame([stats])], ignore_index=True)\n",
        "    df = df[[\"precision\", \"recall\", \"f1-score\", \"support\", \"timestamp\", \"Experiment Name\"]]\n",
        "    markdown_table = tabulate(df, headers='keys', tablefmt='pipe', showindex=False, floatfmt=(\".3g\"), intfmt=\",\")\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(markdown_table)\n",
        "\n",
        "def evaluate_model(y_test, predictions, clf):\n",
        "    stats = classification_report(y_test, predictions, output_dict=True)\n",
        "    print(classification_report(y_test, predictions))\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    cmp = ConfusionMatrixDisplay(\n",
        "        confusion_matrix(y_test, predictions),\n",
        "        display_labels=clf.classes_,\n",
        "    )\n",
        "\n",
        "    cmp.plot(ax=ax)\n",
        "    plt.show()\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCg-mOMsFqGg"
      },
      "outputs": [],
      "source": [
        "experiment_name = input(\"Enter experiment name: \")\n",
        "predictions = clf.predict(X_test)\n",
        "stats = evaluate_model(y_test, predictions, clf)\n",
        "log_experiment_results(experiment_name, stats[\"macro avg\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnLyQ2OPFqGj"
      },
      "source": [
        "## What's next?\n",
        "### Fundamental limitations of the BOW approach\n",
        "1. We're just counting words, which means we don't consider the order of words in a sentence.\n",
        "2. Lots of OOV tokens that are modelled incorrectly, which we don't like.\n",
        "3. We are using every word with equal weight, while some words are more important than others\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJXYYSrNFqGj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Vlerick-MAI-NLP-demo.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
