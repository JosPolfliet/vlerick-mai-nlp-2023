{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5iPecqWFqGa"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4KpuOwlGkBY"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vW9svTE289D"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1qmVm97FqGc"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://media.githubusercontent.com/media/JosPolfliet/vlerick-mai-nlp-2023/main/DATA/esg_reports.csv\")\n",
    "df[\"subject\"] = df[\"subject\"].fillna(\"Other\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHDlY8EIGJ65"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Iwmp457FqGc"
   },
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fufy_rezFqGc"
   },
   "outputs": [],
   "source": [
    "for i in range(10,15):\n",
    "    print(df.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhPMFwkCFqGd"
   },
   "source": [
    "We'll use the following example sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-8g_XUaFqGd"
   },
   "outputs": [],
   "source": [
    "sentence = df.iloc[67]\n",
    "sentence.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85DhlRx3FqGd"
   },
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Osx-rYCNFqGd"
   },
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(sentence[\"text\"].lower())\n",
    "\n",
    "    # Remove OOV words\n",
    "    mytokens = [ word for word in mytokens if not word.is_oov ]\n",
    "\n",
    "    # Lemmatise + lower case\n",
    "    mytokens = [ word.lemma_.strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Remove stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    return mytokens\n",
    "\n",
    "spacy_tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWxPPWnwFqGd"
   },
   "source": [
    "## Calculate word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVT-2h86FqGe"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokenFreq = Counter()\n",
    "\n",
    "def countWordFrequencies(example):\n",
    "    tokens = spacy_tokenizer(example)\n",
    "    tokenFreq.update(tokens)\n",
    "\n",
    "df.progress_apply(countWordFrequencies, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3rYvurNFqGe"
   },
   "outputs": [],
   "source": [
    "tokenFreq.total()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVH6d5qgFqGe"
   },
   "outputs": [],
   "source": [
    "len(tokenFreq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RFdZ_ugFqGe"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuK6DtSrFqGe"
   },
   "outputs": [],
   "source": [
    "print(\"Most frequent\")\n",
    "print(tokenFreq.most_common(10))\n",
    "print(\"Least frequent\")\n",
    "print(tokenFreq.most_common(VOCAB_SIZE)[-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SqdbXlYFqGe"
   },
   "outputs": [],
   "source": [
    "tokenMap = {k: i+1 for i, (k, _) in enumerate(tokenFreq.most_common(VOCAB_SIZE-1))}\n",
    "tokenMap[\"<oov>\"]=0\n",
    "tokenMapInverse = {v: k for k,v in tokenMap.items()}\n",
    "print(list(tokenMap.items())[0:10])\n",
    "print([tokenMapInverse[i] for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZaaB9ssFqGf"
   },
   "source": [
    "## Simple model - BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Pb_s8Q_FqGf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QoU41ISFFqGf"
   },
   "outputs": [],
   "source": [
    "def embed_sentence_BOW(example):\n",
    "    \"\"\"\n",
    "    Simple BOW\n",
    "    \"\"\"\n",
    "    embedding = np.zeros(VOCAB_SIZE)\n",
    "    tokens = spacy_tokenizer(example)\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            tokenIndex = tokenMap[token]\n",
    "        except KeyError: # Out of vocabulary\n",
    "            tokenIndex = 0\n",
    "        embedding[tokenIndex] = embedding[tokenIndex] + 1\n",
    "    return embedding\n",
    "\n",
    "sentence = df.iloc[302]\n",
    "sentence_embedding = embed_sentence_BOW(sentence)\n",
    "\n",
    "print(sentence_embedding)\n",
    "print(sentence_embedding.shape)\n",
    "print(sentence)\n",
    "print(\"What this means: \")\n",
    "\n",
    "for i in range(VOCAB_SIZE):\n",
    "    if sentence_embedding[i]:\n",
    "        print(f\"Token {i} '{tokenMapInverse[i]}' occurs {sentence_embedding[i]} time in sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cQLF8SbFqGf"
   },
   "outputs": [],
   "source": [
    "df[\"embedding\"] = df.progress_apply(embed_sentence_BOW, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0UhlqFXFqGf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hPl_UUaFqGf"
   },
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByNqWPoNFqGf"
   },
   "outputs": [],
   "source": [
    "X = np.stack(df[\"embedding\"])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGDEwmSiFqGf"
   },
   "outputs": [],
   "source": [
    "X[302:303,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JV-98p6_FqGf"
   },
   "outputs": [],
   "source": [
    "y = np.array(df[\"subject\"])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1TUcHn1FqGf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4aC67giFqGg"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvGxXVwhFqGg"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22141)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjPUxVzoFqGg"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=22141, class_weight=\"balanced\", n_estimators=300)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhE9VoKTFqGg"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IXH1un-FqGg"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (ConfusionMatrixDisplay, classification_report,\n",
    "                             confusion_matrix)\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "from tabulate import tabulate\n",
    "\n",
    "def log_experiment_results(experiment_name, stats, filename=\"experiment_log.md\"):\n",
    "    \"\"\"\n",
    "    Appends experiment results and statistics to a markdown log file.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment_name: str, the name of the experiment\n",
    "    - stats: dict, a dictionary containing the statistics to log\n",
    "    - filename: str, the path to the log file\n",
    "    \"\"\"\n",
    "    stats[\"timestamp\"] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "    stats[\"Experiment Name\"] = experiment_name\n",
    "    try:\n",
    "\n",
    "        df = pd.read_table(filename, sep=\"|\", skipinitialspace=True).drop(0)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError, pd.errors.ParserError):\n",
    "        df = pd.DataFrame(columns=list(stats.keys()))\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame([stats])], ignore_index=True)\n",
    "    df = df[[\"precision\", \"recall\", \"f1-score\", \"support\", \"timestamp\", \"Experiment Name\"]]\n",
    "    markdown_table = tabulate(df, headers='keys', tablefmt='pipe', showindex=False, floatfmt=(\".3g\"), intfmt=\",\")\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(markdown_table)\n",
    "\n",
    "def evaluate_model(y_test, predictions, clf):\n",
    "    stats = classification_report(y_test, predictions, output_dict=True)\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    cmp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix(y_test, predictions),\n",
    "        display_labels=clf.classes_,\n",
    "    )\n",
    "\n",
    "    cmp.plot(ax=ax)\n",
    "    plt.show()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCg-mOMsFqGg"
   },
   "outputs": [],
   "source": [
    "experiment_name = input(\"Enter experiment name: \")\n",
    "predictions = clf.predict(X_test)\n",
    "stats = evaluate_model(y_test, predictions, clf)\n",
    "log_experiment_results(experiment_name, stats[\"macro avg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnLyQ2OPFqGj"
   },
   "source": [
    "## What's next?\n",
    "### Fundamental limitations of the BOW approach\n",
    "1. We're just counting words, which means we don't consider the order of words in a sentence.\n",
    "2. Lots of OOV tokens that are modelled incorrectly, which we don't like.\n",
    "3. We are using every word with equal weight, while some words are more important than others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJXYYSrNFqGj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Vlerick-MAI-NLP-demo.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
