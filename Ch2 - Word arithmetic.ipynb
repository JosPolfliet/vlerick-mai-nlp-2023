{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting wordfreq\n",
      "  Downloading wordfreq-3.0.2-py3-none-any.whl (56.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.7.6 in /opt/homebrew/lib/python3.10/site-packages (from wordfreq) (2022.9.13)\n",
      "Requirement already satisfied: langcodes>=3.0 in /opt/homebrew/lib/python3.10/site-packages (from wordfreq) (3.3.0)\n",
      "Collecting ftfy>=6.1\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Collecting msgpack>=1.0\n",
      "  Downloading msgpack-1.0.4-cp310-cp310-macosx_11_0_arm64.whl (69 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m69.5/69.5 kB\u001b[0m \u001b[31m725.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /opt/homebrew/lib/python3.10/site-packages (from ftfy>=6.1->wordfreq) (0.2.5)\n",
      "Installing collected packages: msgpack, ftfy, wordfreq\n",
      "Successfully installed ftfy-6.1.1 msgpack-1.0.4 wordfreq-3.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "8vW9svTE289D"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from wordfreq import zipf_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are only available in larger models, so let's download that first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting en-core-web-md==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /opt/homebrew/lib/python3.10/site-packages (from en-core-web-md==3.4.1) (3.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.23.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (65.4.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.6.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/homebrew/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/homebrew/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/homebrew/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.1.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/homebrew/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/homebrew/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "mWDrpxDk2_r2"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "s = \"It's not about the money (only $20.15), it's about sending a message :). üöÄüíéüôå\"\n",
    "doc = nlp(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc[1]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_id(word: str):\n",
    "    return nlp.vocab.strings[word]\n",
    "\n",
    "def get_vector(word: str):\n",
    "    return nlp.vocab.vectors[vocab_id(word)]\n",
    "\n",
    "def get_token(word: str):\n",
    "    return nlp(word)[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41661593317985535"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_token(\"man\").similarity(get_token(\"king\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat cat 1.0\n",
      "cat lion 0.3854507803916931\n",
      "cat pet 0.732966423034668\n",
      "lion cat 0.3854507803916931\n",
      "lion lion 1.0\n",
      "lion pet 0.20031583309173584\n",
      "pet cat 0.732966423034668\n",
      "pet lion 0.20031583309173584\n",
      "pet pet 1.0\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'cat lion pet')\n",
    "\n",
    "for t1 in tokens:\n",
    "    for t2 in tokens:\n",
    "        print(t1.text,t2.text,t1.similarity(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "castle, castle, 1.000\n",
      "castle, king, 0.494\n",
      "castle, student, -0.021\n",
      "castle, pub, 0.305\n",
      "king, castle, 0.494\n",
      "king, king, 1.000\n",
      "king, student, 0.201\n",
      "king, pub, 0.014\n",
      "student, castle, -0.021\n",
      "student, king, 0.201\n",
      "student, student, 1.000\n",
      "student, pub, -0.062\n",
      "pub, castle, 0.305\n",
      "pub, king, 0.014\n",
      "pub, student, -0.062\n",
      "pub, pub, 1.000\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'castle king student pub')\n",
    "\n",
    "for t1 in tokens:\n",
    "    for t2 in tokens:\n",
    "        print(f\"{t1.text}, {t2.text}, {t1.similarity(t2):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find most similar words to a given vector\n",
    "Documentation https://spacy.io/api/vectors#most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(vec: np.ndarray):\n",
    "    # vec = vector(word)\n",
    "\n",
    "    vocab_ids = nlp.vocab.vectors.most_similar(np.asarray([vec]), n=100)\n",
    "    words =  [nlp.vocab.strings[w] for w in vocab_ids[0][0]]\n",
    "\n",
    "    return [w for w in words if get_token(w).is_alpha & (zipf_frequency(w, \"en\", wordlist='small', minimum=1)> 3)][0:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doctor',\n",
       " 'physician',\n",
       " 'psychiatrist',\n",
       " 'doctors',\n",
       " 'dentist',\n",
       " 'nurse',\n",
       " 'pharmacist',\n",
       " 'pediatrician',\n",
       " 'surgeon',\n",
       " 'proctor',\n",
       " 'dermatologist',\n",
       " 'veterinarian',\n",
       " 'midwife',\n",
       " 'psychiatrists',\n",
       " 'therapist',\n",
       " 'neurologist',\n",
       " 'clinic',\n",
       " 'medic',\n",
       " 'Pediatrician',\n",
       " 'physicians']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar(get_vector(\"doctor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doctor',\n",
       " 'pediatrician',\n",
       " 'nurse',\n",
       " 'midwife',\n",
       " 'physician',\n",
       " 'Pediatrician',\n",
       " 'dermatologist',\n",
       " 'dentist',\n",
       " 'therapist',\n",
       " 'Dermatologist',\n",
       " 'clinic',\n",
       " 'pharmacist',\n",
       " 'doctors',\n",
       " 'Midwife',\n",
       " 'pediatrics',\n",
       " 'psychiatrist',\n",
       " 'veterinarian',\n",
       " 'pediatric',\n",
       " 'neurologist',\n",
       " 'Physician']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar(get_vector(\"doctor\") - get_vector(\"man\") + get_vector(\"woman\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king',\n",
       " 'princess',\n",
       " 'princesses',\n",
       " 'princes',\n",
       " 'prince',\n",
       " 'kings',\n",
       " 'queen',\n",
       " 'consort',\n",
       " 'Mcqueen',\n",
       " 'mcqueen',\n",
       " 'monarch',\n",
       " 'ruler',\n",
       " 'rulers',\n",
       " 'Princesses',\n",
       " 'thrones',\n",
       " 'kingdom',\n",
       " 'monarchs',\n",
       " 'throne',\n",
       " 'kingdoms',\n",
       " 'royal']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar(get_vector(\"king\") - get_vector(\"man\") + get_vector(\"girl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['student',\n",
       " 'campus',\n",
       " 'school',\n",
       " 'dormitory',\n",
       " 'university',\n",
       " 'castle',\n",
       " 'students',\n",
       " 'classmate',\n",
       " 'pupil',\n",
       " 'teacher',\n",
       " 'classroom',\n",
       " 'headmaster',\n",
       " 'undergraduate',\n",
       " 'college',\n",
       " 'Dormitory',\n",
       " 'undergraduates',\n",
       " 'gymnasium',\n",
       " 'schoolboy',\n",
       " 'pupils',\n",
       " 'highschool']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar(get_vector(\"castle\") - get_vector(\"royalty\") + get_vector(\"student\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You could, for instance, simply add two vectors together to get a new vector, and search for the nearest word. The results, as we have seen, often made a shocking amount of sense:\n",
    "Czech + currency = korunaVietnam + capital = Hanoi\n",
    "German + airlines = Lufthansa\n",
    "French + actress = Juliette Binoche\n",
    "\n",
    "And you could subtract words, too. This meant‚Äîincredibly‚Äîyou could produce ‚Äúanalogies‚Äù by getting the ‚Äúdifference‚Äù between two words and then ‚Äúadding‚Äù it to a third.66\n",
    "These analogies suggested that the embeddings had captured geography:\n",
    "\n",
    "Berlin ‚àí Germany + Japan = Tokyo\n",
    "\n",
    "And grammar:\n",
    "bigger ‚àí big + cold = colder\n",
    "\n",
    "And cuisine:\n",
    "sushi ‚àí Japan + Germany = bratwurst\n",
    "\n",
    "And science:\n",
    "Cu ‚àí copper + gold = Au\n",
    "\n",
    "And tech:\n",
    "Windows ‚àí Microsoft + Google = Android\n",
    "\n",
    "And sports:\n",
    "Montreal Canadiens ‚àí Montreal + Toronto = Toronto Maple Leafs67\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "- Library for embedding exploration: https://github.com/koaning/whatlies/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "nlpdemystified-preprocessing.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
