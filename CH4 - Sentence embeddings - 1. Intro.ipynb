{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cd0adc-60ff-4e8c-9a36-43f59ce578de",
   "metadata": {},
   "source": [
    "# Sentence embeddings\n",
    "We will mainly use `sentence-transformers`, which is a dedicated package from Hugging Face ðŸ¤—. \n",
    "\n",
    "Relevant documentation\n",
    "- Semantic textual similarity https://www.sbert.net/docs/usage/semantic_textual_similarity.html\n",
    "- Semantic search https://www.sbert.net/examples/applications/semantic-search/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f070d4c-1c9b-4af9-9b22-d357e07c0ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers faiss-cpu langchain langchain-community \"unstructured[all-docs]\" openai nest-asyncio streamlit jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed070c3-50f0-4d3c-ad73-4b5fef706279",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### From word embeddings to sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10fd65a-d607-4a2f-8e46-c07bb71ec11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee6e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Sentences we want to encode. Example:\n",
    "sentence = ['This framework generates embeddings for each input sentence']\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "embedding = model.encode(sentence)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0ba9e8-82a7-458b-97ab-d9af7f2555a5",
   "metadata": {},
   "source": [
    "See, a sentence embedding is just a vector, just like a word embedding. That means we can also calculate similarities in a similar way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5113acd-53a0-47bd-a7b6-d1f072b0baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome!']\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',\n",
    "              'My plants look a bit sick, could it be bitrot?',\n",
    "              'The new movie is so great!']\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f74a88-c711-4b1b-a7cc-8a7a473c2478",
   "metadata": {},
   "source": [
    "## Semantic search and retrieval\n",
    "\n",
    "The idea behind semantic search is to embed all entries in your corpus, whether they be sentences, paragraphs, or documents, into a vector space.\n",
    "\n",
    "At search time, the query is embedded into the same vector space and the closest embeddings from your corpus are found. These entries should have a high semantic overlap with the query.\n",
    "\n",
    "\n",
    "![title](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/SemanticSearch.png\n",
    ")\n",
    "\n",
    "Instead of trying to build a semantic search engine from first principles, we'll use `langchain`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08117fa3-80b8-4138-bf75-d540930084a7",
   "metadata": {},
   "source": [
    "## [Don't run this again] Crawl the Vlerick website using Apify\n",
    "\n",
    "The following code crawls the Vlerick website so we have some text to model. It's just example code. \n",
    "\n",
    "Langchain supports more than 100 integrations, so depending on where you find interesting data you'll need to use something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bd23b-1ccd-4e2c-8ca5-cc5f95381879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.utilities import ApifyWrapper\n",
    "# import os\n",
    "\n",
    "# os.environ[\"APIFY_API_TOKEN\"] = \"\"\n",
    "\n",
    "# apify = ApifyWrapper()\n",
    "# # Call the Actor to obtain text from the crawled webpages\n",
    "# loader = apify.call_actor(\n",
    "#     actor_id=\"apify/website-content-crawler\",\n",
    "#     run_input={\n",
    "#         \"startUrls\": [{\"url\": \"https://www.vlerick.com/en/\"}]\n",
    "#     },\n",
    "#     dataset_mapping_function=lambda item: Document(\n",
    "#         page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\n",
    "#     ),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3117d9c0-64a1-42ef-80e5-0beec692644a",
   "metadata": {},
   "source": [
    "## Create new vector store and embed all documents\n",
    "Source: https://python.langchain.com/docs/expression_language/cookbook/retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b604e3e2-ff3f-4e40-ba93-1d55d5698bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load all documents\n",
    "# Adapt this code to your own source of data.\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader, JSONLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac200d-b25e-445d-92da-9539609e70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "loader = DirectoryLoader('/Users/jospolfliet/src/vlerick/DATA/MAI-2023 dump/', silent_errors=True)\n",
    "course_docs = loader.load()\n",
    "\n",
    "print(f\"Number of documents {len(course_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e88ba1-b915-46ef-88af-63fbd50f4153",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_docs[9:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd20bfb-5cc4-475b-a6b8-e6555e3f84ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.document_loaders import ApifyDatasetLoader\n",
    "from langchain_community.document_loaders.base import Document\n",
    "\n",
    "loader = ApifyDatasetLoader(\n",
    "    dataset_id=\"RcArHfVs80xOg9IKs\",\n",
    "    dataset_mapping_function=lambda dataset_item: Document(\n",
    "        page_content=dataset_item[\"text\"], metadata={\"source\": dataset_item[\"url\"]}\n",
    "    ),\n",
    ")\n",
    "website_docs = loader.load()\n",
    "print(f\"Number of documents {len(website_docs)}\")\n",
    "website_docs = [doc for doc in website_docs if not doc.page_content.startswith(\"Your choice regarding cookies on this site\")]\n",
    "print(f\"Number of non-trivial documents {len(website_docs)}\")\n",
    "website_docs[5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfc75d6-a218-4c49-8949-35c8fed54a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(course_docs + website_docs)\n",
    "documents[0]\n",
    "print(f\"Number of chunks {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472d4428-d9b8-4b62-be3d-cbed5058fe6c",
   "metadata": {},
   "source": [
    "### Embed into a vector store - and cache the results\n",
    "We got a decent store of data loaded into memory now. Next thing we need to do is calculate sentence embeddings. \n",
    "We'll use simple, reasonably fast embeddings that we can calculate locally withouting requiring an expensive GPU or cloud service like OpenAI's GPTx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed4206-743d-4e72-b471-e2e300642645",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# to test, use query_result = embeddings.embed_query(\"My text\")\n",
    "\n",
    "if True: # change to True if you want to (re)create your store   \n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents, embedding=embeddings\n",
    "    )\n",
    "    # store because this is slow\n",
    "    vectorstore.save_local(\"vectorstore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c74ddc-4144-4ff6-a4bf-4f64b9ef1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.load_local(\"vectorstore\", embeddings)\n",
    "vectorstore.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a80908f-1696-461d-98ca-f973038139dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "\n",
    "retriever = vectorstore.as_retriever(k=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf70a1-b150-47bc-af58-9de50d0cd07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(s):\n",
    "    results = retriever.get_relevant_documents(s)\n",
    "    for doc in results:\n",
    "        print(\"#\"*100)\n",
    "        print(doc.metadata[\"source\"])\n",
    "        print(\"#\"*100)\n",
    "        print(doc.page_content)\n",
    "q(\"stochastic gradient descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b51e1d-d731-4d93-aff5-4a191740e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "q(\"what type of awards does vlerick give\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba6dee-4871-42f4-a22e-371437e57d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da656ab0-eda2-424a-af9f-43d0eb609544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
