{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "43131da9-de8d-409b-8535-a1c2fafd7440",
      "metadata": {
        "id": "43131da9-de8d-409b-8535-a1c2fafd7440"
      },
      "source": [
        "# Let's build a language model\n",
        "This lecture is inspired by and uses some code from Andrej Karpathy's excellent lecture series on NLP.\n",
        "The original Transformers lecture can be watched on [Youtube](https://www.youtube.com/watch?v=kCc8FmEb1nY). I encourage you to do so: having two people explain the same usually benefits understanding a lot.\n",
        "\n",
        "To broaden your understanding even further, you can also study an alternative implementation doing something similar, noting the differences. For example [GPT in 60 lines of code](https://jaykmody.com/blog/gpt-from-scratch/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff12b39f-4545-4ada-9924-486cdf827872",
      "metadata": {
        "id": "ff12b39f-4545-4ada-9924-486cdf827872",
        "outputId": "8ebd978d-d2f8-4811-acf7-7c9905b9ea34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.1.1)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "048dcacb-bb28-4fdc-9646-8647fde657c1",
      "metadata": {
        "id": "048dcacb-bb28-4fdc-9646-8647fde657c1",
        "outputId": "52314840-19de-4a6b-87d3-9a3aaed28753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-02 17:30:12--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-01-02 17:30:12 (20.4 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "   7241 \n",
            "   5437 the\n",
            "   4403 I\n",
            "   3923 to\n",
            "   3678 and\n",
            "   3275 of\n",
            "   2677 my\n",
            "   2610 a\n",
            "   2130 you\n",
            "   2073 in\n",
            "   1812 that\n",
            "   1801 And\n",
            "   1768 is\n",
            "   1631 not\n",
            "   1564 with\n",
            "   1493 your\n",
            "   1489 be\n",
            "   1391 his\n",
            "   1381 for\n",
            "   1280 have\n",
            "   1189 it\n",
            "   1149 he\n",
            "   1122 this\n",
            "   1111 me\n",
            "   1093 thou\n",
            "   1025 as\n",
            "    953 thy\n",
            "    865 but\n",
            "    858 will\n",
            "    842 The\n",
            "    800 To\n",
            "    715 shall\n",
            "    712 by\n",
            "    710 him\n",
            "    702 our\n",
            "    697 so\n",
            "    676 all\n",
            "    670 are\n",
            "    658 we\n",
            "    615 That\n",
            "    608 do\n",
            "    604 her\n",
            "    588 no\n",
            "    557 what\n",
            "    537 But\n",
            "    531 from\n",
            "    523 on\n",
            "    504 good\n",
            "    494 if\n",
            "    492 at\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!cat input.txt | tr ' ' '\\n' | sort | uniq -c | sort -nr | head -n50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6c7ce845-9059-420a-8ad0-2e08a3e568ff",
      "metadata": {
        "id": "6c7ce845-9059-420a-8ad0-2e08a3e568ff",
        "outputId": "ff1f9d1d-8c3c-43de-8bb9-2ea51a5cad1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ],
      "source": [
        "# Read all text.\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(text[0:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e065cd8a-f0ca-44d6-94ea-914f98a0b829",
      "metadata": {
        "id": "e065cd8a-f0ca-44d6-94ea-914f98a0b829"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0d445007-33f4-46bf-a305-fd0c15c8aaeb",
      "metadata": {
        "id": "0d445007-33f4-46bf-a305-fd0c15c8aaeb"
      },
      "source": [
        "## Build a simple next-character-prediction model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5123610b-4b8a-4850-96b4-ceba0a3933d4",
      "metadata": {
        "id": "5123610b-4b8a-4850-96b4-ceba0a3933d4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "942c94ae-8777-4bde-8d25-93fc0db16bd1",
      "metadata": {
        "id": "942c94ae-8777-4bde-8d25-93fc0db16bd1"
      },
      "source": [
        "### Tokenizer\n",
        "The tokenizer is super simple here: it's all characters that occur in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "edcda719-ec6b-4eef-bcca-0299ee3f4c3d",
      "metadata": {
        "id": "edcda719-ec6b-4eef-bcca-0299ee3f4c3d"
      },
      "outputs": [],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a character, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a99c2ced-9661-4af9-a591-87dda2378e00",
      "metadata": {
        "id": "a99c2ced-9661-4af9-a591-87dda2378e00",
        "outputId": "fa1e09e1-9684-4be3-eb18-815128047e77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
          ]
        }
      ],
      "source": [
        "print(stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0e8903b1-39c0-4df4-ab59-597a00e90fe9",
      "metadata": {
        "id": "0e8903b1-39c0-4df4-ab59-597a00e90fe9",
        "outputId": "fba775e2-df8a-48d6-89b3-53e95268d5f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "[[31], [54], [43], [39], [49], [6], [1], [57], [54], [43], [39], [49], [2]]\n"
          ]
        }
      ],
      "source": [
        "print(text[0:100])\n",
        "print([encode(c) for c in \"Speak, speak!\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "948228d1-9572-4796-b18c-0fc9b374c20d",
      "metadata": {
        "id": "948228d1-9572-4796-b18c-0fc9b374c20d"
      },
      "source": [
        "### Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9824e61d-abbc-4fb4-880d-487e7b5569e4",
      "metadata": {
        "id": "9824e61d-abbc-4fb4-880d-487e7b5569e4",
        "outputId": "62eea691-e1bc-42aa-9bbf-98bc5cd56884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We'll use input 'N' to predict the next character 'a'.\n"
          ]
        }
      ],
      "source": [
        "# We want to predict the next character\n",
        "\n",
        "ix=1339\n",
        "\n",
        "# example 1.1:\n",
        "print(f\"We'll use input '{text[ix:ix+1]}' to predict the next character '{text[ix+1:ix+2]}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2f3082c8-4e79-4eab-b66d-7967231ec576",
      "metadata": {
        "id": "2f3082c8-4e79-4eab-b66d-7967231ec576",
        "outputId": "e4742636-dfcf-478c-fc7d-21c5644493ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We'll use input 'N' to predict the next character 'a'.\n",
            "We'll use input 'Na' to predict the next character 'y'.\n",
            "We'll use input 'Nay' to predict the next character ','.\n",
            "We'll use input 'Nay,' to predict the next character ' '.\n",
            "We'll use input 'Nay, ' to predict the next character 'b'.\n",
            "We'll use input 'Nay, b' to predict the next character 'u'.\n",
            "We'll use input 'Nay, bu' to predict the next character 't'.\n",
            "We'll use input 'Nay, but' to predict the next character ' '.\n",
            "We'll use input 'Nay, but ' to predict the next character 's'.\n",
            "We'll use input 'Nay, but s' to predict the next character 'p'.\n",
            "We'll use input 'Nay, but sp' to predict the next character 'e'.\n",
            "We'll use input 'Nay, but spe' to predict the next character 'a'.\n",
            "We'll use input 'Nay, but spea' to predict the next character 'k'.\n"
          ]
        }
      ],
      "source": [
        "for j in range(13):\n",
        "    print(f\"We'll use input '{text[ix:ix+j+1]}' to predict the next character '{text[ix+j+1:ix+j+2]}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac8f15f9-3e1e-49c8-93a1-88aa28c0989c",
      "metadata": {
        "id": "ac8f15f9-3e1e-49c8-93a1-88aa28c0989c"
      },
      "source": [
        "So one example of 13 characters is actually 13 sub examples!\n",
        "\n",
        "We start with zero-length sequences because we want to make the model robust to starting from scratch.\n",
        "\n",
        "Let's implement this in a structural way so that this whole data generation thing happens automatically. We'll also add some batches because we want to make sure the GPU's are flooded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ec4813d6-7271-4ba6-a204-c05933a4b87d",
      "metadata": {
        "id": "ec4813d6-7271-4ba6-a204-c05933a4b87d"
      },
      "outputs": [],
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "block_size=13\n",
        "batch_size=8\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # note the random sampling from the original data\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0292dcd1-95bf-4b22-a2f4-b2391f33634c",
      "metadata": {
        "id": "0292dcd1-95bf-4b22-a2f4-b2391f33634c"
      },
      "source": [
        "Let's try it out and see if it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4a9b031a-0ac1-4ff3-afdb-5b1dd369451f",
      "metadata": {
        "id": "4a9b031a-0ac1-4ff3-afdb-5b1dd369451f",
        "outputId": "2c3e1987-fb2e-468a-baec-b17968a3642c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----\n",
            "x vector:  tensor([53, 59, 56,  1, 46, 47, 45, 46,  1, 42, 43, 57, 41])\n",
            "x text:  our high desc\n",
            "y vector:  tensor([53, 59, 56,  1, 46, 47, 45, 46,  1, 42, 43, 57, 41])\n",
            "y text:  ur high desce\n",
            "----\n",
            "x vector:  tensor([43, 57, 57,  1, 44, 56, 43, 55, 59, 43, 52, 58,  1])\n",
            "x text:  ess frequent \n",
            "y vector:  tensor([43, 57, 57,  1, 44, 56, 43, 55, 59, 43, 52, 58,  1])\n",
            "y text:  ss frequent t\n",
            "----\n",
            "x vector:  tensor([40, 51, 47, 57, 57, 47, 53, 52,  1, 61, 47, 50, 50])\n",
            "x text:  bmission will\n",
            "y vector:  tensor([40, 51, 47, 57, 57, 47, 53, 52,  1, 61, 47, 50, 50])\n",
            "y text:  mission will \n"
          ]
        }
      ],
      "source": [
        "x,y = get_batch(\"train\")\n",
        "for idx in range(3):\n",
        "    xi=x[idx]\n",
        "    yi=y[idx]\n",
        "    print(\"----\")\n",
        "    print(f\"x vector: \", xi)\n",
        "    print(\"x text: \", \"\".join([itos[int(i)] for i in xi]))\n",
        "    print(f\"y vector: \", xi)\n",
        "    print(\"y text: \", \"\".join([itos[int(i)] for i in yi]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11ebde57-7be5-4b66-a211-5653dc4ed2ad",
      "metadata": {
        "id": "11ebde57-7be5-4b66-a211-5653dc4ed2ad"
      },
      "source": [
        "We are randomly sampling snippets of text of `block_size` long. Then, for each of those snippets we are creating multiple examples: for every character we want to predict the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d6c64de-2060-45c4-9bbc-c9893352c787",
      "metadata": {
        "id": "1d6c64de-2060-45c4-9bbc-c9893352c787"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a33ebf59-7f83-4193-833f-7601ae6e3508",
      "metadata": {
        "id": "a33ebf59-7f83-4193-833f-7601ae6e3508"
      },
      "outputs": [],
      "source": [
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecb89fca-2af9-4dda-8ea2-cee6c8f42c34",
      "metadata": {
        "id": "ecb89fca-2af9-4dda-8ea2-cee6c8f42c34"
      },
      "source": [
        "#### What does a random model do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b32bc7c9-4c59-4a82-9866-f2702d18e3fc",
      "metadata": {
        "id": "b32bc7c9-4c59-4a82-9866-f2702d18e3fc",
        "outputId": "4ddfdeb1-5bfd-449a-8569-25fc28d511d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "gMy$oSimjXp!KF aimhA3MrTzArOo,o,$?IMh,LnZ3Md.&Nvmr;\n",
            "IK?aZU rbq,PKjoAg3'- a?rWmktIxT.n!$TiVaiI,oqByQYWGajJSTNvsk.&JjHy?GAkGCvo ' rCX$hv&L!;skPYbbJvi--\n",
            "\n",
            "IBvGdkpkqXjJvlpB!B.rdDNLUoyskPpkZF;KLn!emAbuXEcmOWZ3XqCXdu?MP-Temuxrka.mtgM;$?jgOj $qm,LX;YX!;PtJjOCX\n",
            "un&;ed;aNslbmskB.z nCgU3fjV-,?,$dr:dC;aDZ3M;wNXUDjOJ$TM;QNfc:YnDNIGDJkSEoflXFmhphCXFCXFskjFNTfm?BGEUDtK?'K:Go!X skzrYFOd;M-Gn,Z\n",
            "-hqTbrb YDzm?AxIGHA;Q$;\n",
            "NK:dBfaQeW.vyLADAPtgaZrWe;ISoLlX\n",
            "uJvbUlnO\n",
            "-zmsKaiO!;r;hFoRtK?MJRCKBcPeDjqmrdZ-SUYsvL:xGa3MeveYK\n"
          ]
        }
      ],
      "source": [
        "# generate from random model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d5d757-9650-42a0-95b6-630b92c45244",
      "metadata": {
        "id": "50d5d757-9650-42a0-95b6-630b92c45244"
      },
      "source": [
        "### Add a simple way to evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9e75f529-665d-433b-9839-60ec008d0882",
      "metadata": {
        "id": "9e75f529-665d-433b-9839-60ec008d0882"
      },
      "outputs": [],
      "source": [
        "# Add some way to evaluate\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11afab5f-dad6-4570-b1dd-c00d3711fa18",
      "metadata": {
        "id": "11afab5f-dad6-4570-b1dd-c00d3711fa18"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3ce63fbd-94ce-405b-964e-23420f3169ef",
      "metadata": {
        "id": "3ce63fbd-94ce-405b-964e-23420f3169ef",
        "outputId": "63e370b2-7ba4-4f75-80b2-ce7b2f745cb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.7481, val loss 4.7390\n",
            "step 2000: train loss 2.4627, val loss 2.4891\n",
            "step 4000: train loss 2.4660, val loss 2.4983\n",
            "step 6000: train loss 2.4571, val loss 2.4937\n",
            "step 8000: train loss 2.4504, val loss 2.4842\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# create a PyTorch optimizer\n",
        "learning_rate = 1e-2\n",
        "max_iters = 10000\n",
        "eval_iters=500\n",
        "eval_interval=max_iters//5\n",
        "def run_training_loop():\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for iter in range(max_iters):\n",
        "\n",
        "        # every once in a while evaluate the loss on train and val sets\n",
        "        if iter % eval_interval == 0:\n",
        "            losses = estimate_loss()\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # sample a batch of data\n",
        "        xb, yb = get_batch('train')\n",
        "\n",
        "        # evaluate the loss\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "run_training_loop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "366d0183-752f-4ea7-906c-efe0a42ddeef",
      "metadata": {
        "id": "366d0183-752f-4ea7-906c-efe0a42ddeef"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "678e96b1-4789-41e1-afb3-5c54373fa618",
      "metadata": {
        "id": "678e96b1-4789-41e1-afb3-5c54373fa618"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d770648f-c8de-430b-845b-279253e59739",
      "metadata": {
        "id": "d770648f-c8de-430b-845b-279253e59739",
        "outputId": "5e72509d-c59d-400d-c4f2-779e536397a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ININGO:\n",
            "\n",
            "AREEN and dathingurus ghathy hen:\n",
            "Th e, tselorintrswe:\n",
            "\n",
            "\n",
            "Tril.\n",
            "'lor:\n",
            "Thocthodig,\n",
            "F bsththt ngous hue helletesprme.\n",
            "F tiz thea amed t s, tel\n",
            "Th thal uiathif we frof wat byo s de blabye f uncotoud psh m;\n",
            "A anor nenoulang de sesasthinoures whe deid tel s t ppe wey, homy thearp d y a, t n imalerk wirmow tont hon, juaithid avepasorer RDo stome wre t blly,\n",
            "\n",
            "An daunden'shafe'sst de me hea y?\n",
            "Tharust gohenoou l ms ma th ayold ckeaceperie yo s prend t pe tay y;\n",
            "\n",
            "Fo;\n",
            "Myo bsottis s mean bul yoved \n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eabb72f-05ac-4fe8-9c4d-946ea99e1bb5",
      "metadata": {
        "id": "5eabb72f-05ac-4fe8-9c4d-946ea99e1bb5"
      },
      "source": [
        "This is not really recognizable text yet, but you can tell that the model is doing something: it tries to make things that have a similar number of characters as words, sentences, punctuation, ...\n",
        "From time to time, some actual common words like \"The\", \"And\" might already appear."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "926a3d88-593a-4317-896d-49e533fb62b5",
      "metadata": {
        "id": "926a3d88-593a-4317-896d-49e533fb62b5"
      },
      "source": [
        "# Adding the first block of Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0ce17cf8-659f-4e5c-a169-fdb9fed8a58a",
      "metadata": {
        "id": "0ce17cf8-659f-4e5c-a169-fdb9fed8a58a"
      },
      "outputs": [],
      "source": [
        "head_size = 32\n",
        "n_embd = 32\n",
        "dropout=0.2\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1)  # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei * k.shape[-1]**-0.5 # keep everything in the same scale.\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) - make sure future tokens aren't accessible\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T) - scale everything nicely\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class OneHeadedModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        self.head = Head(head_size)\n",
        "\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.head(x) # (B,T,C)\n",
        "\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = OneHeadedModel()\n",
        "m = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8420d584-0998-4171-b615-93d289dfd50b",
      "metadata": {
        "id": "8420d584-0998-4171-b615-93d289dfd50b",
        "outputId": "7a87c807-a18a-4ec7-ad58-e3db1de9b2a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2254, val loss 4.2273\n",
            "step 2000: train loss 2.5708, val loss 2.5677\n",
            "step 4000: train loss 2.4899, val loss 2.4992\n",
            "step 6000: train loss 2.4570, val loss 2.4560\n",
            "step 8000: train loss 2.4325, val loss 2.4440\n"
          ]
        }
      ],
      "source": [
        "# Same training loop as before\n",
        "\n",
        "learning_rate = 1e-3\n",
        "max_iters = 10000\n",
        "eval_interval=max_iters//5\n",
        "\n",
        "run_training_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f8ac75ac-4f4f-4757-8585-f0bb1cb9e684",
      "metadata": {
        "id": "f8ac75ac-4f4f-4757-8585-f0bb1cb9e684",
        "outputId": "5f1868f7-5c55-4972-ff65-000cf2ba295a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "d oug ng ato kelt sh withee tharor gybs;UFoutr.\n",
            "CA sobuede Painla int s ou tho ynir your ad, lt dofe at?\n",
            "\n",
            "An:\n",
            "Ne orers aawacpce.\n",
            "\n",
            "HKbelof owoo\n",
            "An.\n",
            "\n",
            "Iy iaretr Ryony.s Hibuchduon cowalds.\n",
            "\n",
            "Ho  Ghas;\n",
            " Bhir waromyoeved chesthadon,\n",
            "Witat hanwes tad nys kee\n",
            "Tasofvof dat hrote dobutn Ifafs wis Ey st, tBus I ert me are thsalithinche, ugomours lane.\n",
            "\n",
            "MHwesthot,\n",
            "ARod ses, Enhy fe\n",
            "Th\n",
            "Dthakim-th metost had\n",
            "Frof witelet ad shanemal pcrely a\n",
            "IONolake. ProrRd\n",
            "fnd,ldo ri uspu, so nWas mipre!atishat tayor lne Ho\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b759d4d8-da82-4717-b95a-1fea149f9c70",
      "metadata": {
        "id": "b759d4d8-da82-4717-b95a-1fea149f9c70"
      },
      "source": [
        "## Adding multiple heads, and finish the whole block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "067fa56c-2488-4a55-a932-07fd91229250",
      "metadata": {
        "id": "067fa56c-2488-4a55-a932-07fd91229250"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "fb9f8c45-f383-43c1-b473-f888c7edb32b",
      "metadata": {
        "id": "fb9f8c45-f383-43c1-b473-f888c7edb32b",
        "outputId": "0a22a12b-c255-4ef8-fe00-68e2a0ed7194",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.017313 M parameters\n"
          ]
        }
      ],
      "source": [
        "n_layer=1\n",
        "n_head=4\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b08403-e2ab-4271-b1db-09d4bd002dd1",
      "metadata": {
        "id": "11b08403-e2ab-4271-b1db-09d4bd002dd1",
        "outputId": "cc1570b2-01e7-40ee-f238-de3bf2fca6a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1852, val loss 4.1852\n",
            "step 2000: train loss 2.2703, val loss 2.2972\n"
          ]
        }
      ],
      "source": [
        "run_training_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f0556cf-3b1b-4041-948e-b74bd3ce7399",
      "metadata": {
        "id": "2f0556cf-3b1b-4041-948e-b74bd3ce7399"
      },
      "outputs": [],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c50c10e-b462-4095-996a-4f0921853f28",
      "metadata": {
        "id": "4c50c10e-b462-4095-996a-4f0921853f28"
      },
      "source": [
        "## Just scale up!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e45df4c-2a1d-417e-a12f-624ace3a490a",
      "metadata": {
        "id": "5e45df4c-2a1d-417e-a12f-624ace3a490a",
        "outputId": "df164b3c-b606-456b-ce13-af1b4fcf4c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.788929 M parameters\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length (here: number of characters) for predictions?\n",
        "learning_rate = 3e-4\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "max_iters = 3000\n",
        "eval_interval=max_iters//10\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11229dfc-3a70-4fd5-971d-481d1282ecfb",
      "metadata": {
        "id": "11229dfc-3a70-4fd5-971d-481d1282ecfb"
      },
      "outputs": [],
      "source": [
        "if False: # Re-train?\n",
        "    # Takes about 15 min on V100.\n",
        "    run_training_loop()\n",
        "    torch.save(m.state_dict(), 'shakespeare_gpt.pth')\n",
        "# step 0: train loss 4.3823, val loss 4.3756\n",
        "# step 500: train loss 1.7285, val loss 1.8738\n",
        "# step 1000: train loss 1.4017, val loss 1.6253\n",
        "# step 1500: train loss 1.2790, val loss 1.5399\n",
        "# step 2000: train loss 1.1935, val loss 1.5008\n",
        "# step 2500: train loss 1.1329, val loss 1.4952\n",
        "# step 3000: train loss 1.0720, val loss 1.5008\n",
        "\n",
        "else:\n",
        "    model = GPTLanguageModel()  # Replace with your model's class\n",
        "    model.load_state_dict(torch.load('shakespeare_gpt.pth', map_location=torch.device(device)))\n",
        "    model.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "442c395e-b990-4da8-a588-7d2d61db7ca1",
      "metadata": {
        "id": "442c395e-b990-4da8-a588-7d2d61db7ca1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "22b8023d-036b-4bd6-8df0-9b2e07c72514",
      "metadata": {
        "id": "22b8023d-036b-4bd6-8df0-9b2e07c72514"
      },
      "source": [
        "## Embeddings\n",
        "Now, we'll dive into the model specifics and see a little trick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa54735e-ef13-45cd-96a7-669ff3b3ef5a",
      "metadata": {
        "id": "fa54735e-ef13-45cd-96a7-669ff3b3ef5a"
      },
      "outputs": [],
      "source": [
        "# Tokenize the words (convert each character to its token ID)\n",
        "words= [\"ROMEO\", \"JULIET\", \"ELIZABETH\",\"queen\", \"love\"]\n",
        "tokenized_words = [[stoi[char] for char in word] for word in words]\n",
        "\n",
        "# Convert to tensor and pad sequences for equal length\n",
        "max_len = max(len(t) for t in tokenized_words)\n",
        "padded_tokens = [t + [0] * (max_len - len(t)) for t in tokenized_words]\n",
        "input_ids = torch.tensor(padded_tokens).to(device)\n",
        "\n",
        "# Forward pass\n",
        "outputs = m(input_ids)\n",
        "\n",
        "# Aggregating character embeddings to get word embeddings\n",
        "# Here, using simple averaging\n",
        "words_emb = outputs[0].mean(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5534cc5-1961-4465-a6a4-2ea3768a996e",
      "metadata": {
        "id": "d5534cc5-1961-4465-a6a4-2ea3768a996e",
        "outputId": "225cfeb8-e304-4995-a1fd-82c768551580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JULIET-ROMEO: 0.94\n",
            "ELIZABETH-ROMEO: 0.71\n",
            "ELIZABETH-JULIET: 0.81\n",
            "queen-ROMEO: 0.36\n",
            "queen-JULIET: 0.39\n",
            "queen-ELIZABETH: 0.43\n",
            "love-ROMEO: 0.63\n",
            "love-JULIET: 0.61\n",
            "love-ELIZABETH: 0.50\n",
            "love-queen: 0.91\n"
          ]
        }
      ],
      "source": [
        "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "for i in range(len(words)):\n",
        "    for j in range(i):\n",
        "        sim = cos(words_emb[i], words_emb[j])\n",
        "        print(f\"{words[i]}-{words[j]}: {sim:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b1ad043-0007-43cd-a73e-48dc557a7f58",
      "metadata": {
        "id": "5b1ad043-0007-43cd-a73e-48dc557a7f58"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3a72399-3b92-4f4a-8bd7-7d8773001b33",
      "metadata": {
        "id": "d3a72399-3b92-4f4a-8bd7-7d8773001b33"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}