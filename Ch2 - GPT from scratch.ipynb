{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "43131da9-de8d-409b-8535-a1c2fafd7440",
      "metadata": {
        "id": "43131da9-de8d-409b-8535-a1c2fafd7440"
      },
      "source": [
        "# Let's build a language model\n",
        "This lecture is inspired by and uses some code from Andrej Karpathy's excellent lecture series on NLP.\n",
        "The original Transformers lecture can be watched on [Youtube](https://www.youtube.com/watch?v=kCc8FmEb1nY). I encourage you to do so: having two people explain the same usually benefits understanding a lot.\n",
        "\n",
        "To broaden your understanding even further, you can also study an alternative implementation doing something similar, noting the differences. For example [GPT in 60 lines of code](https://jaykmody.com/blog/gpt-from-scratch/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ff12b39f-4545-4ada-9924-486cdf827872",
      "metadata": {
        "id": "ff12b39f-4545-4ada-9924-486cdf827872"
      },
      "outputs": [],
      "source": [
        "# !pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "048dcacb-bb28-4fdc-9646-8647fde657c1",
      "metadata": {
        "id": "048dcacb-bb28-4fdc-9646-8647fde657c1",
        "outputId": "30b85b18-083b-49c4-b613-f9b202d87696",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-03 19:50:00--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-01-03 19:50:00 (43.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "   7241 \n",
            "   5437 the\n",
            "   4403 I\n",
            "   3923 to\n",
            "   3678 and\n",
            "   3275 of\n",
            "   2677 my\n",
            "   2610 a\n",
            "   2130 you\n",
            "   2073 in\n",
            "   1812 that\n",
            "   1801 And\n",
            "   1768 is\n",
            "   1631 not\n",
            "   1564 with\n",
            "   1493 your\n",
            "   1489 be\n",
            "   1391 his\n",
            "   1381 for\n",
            "   1280 have\n",
            "   1189 it\n",
            "   1149 he\n",
            "   1122 this\n",
            "   1111 me\n",
            "   1093 thou\n",
            "   1025 as\n",
            "    953 thy\n",
            "    865 but\n",
            "    858 will\n",
            "    842 The\n",
            "    800 To\n",
            "    715 shall\n",
            "    712 by\n",
            "    710 him\n",
            "    702 our\n",
            "    697 so\n",
            "    676 all\n",
            "    670 are\n",
            "    658 we\n",
            "    615 That\n",
            "    608 do\n",
            "    604 her\n",
            "    588 no\n",
            "    557 what\n",
            "    537 But\n",
            "    531 from\n",
            "    523 on\n",
            "    504 good\n",
            "    494 if\n",
            "    492 at\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!cat input.txt | tr ' ' '\\n' | sort | uniq -c | sort -nr | head -n50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6c7ce845-9059-420a-8ad0-2e08a3e568ff",
      "metadata": {
        "id": "6c7ce845-9059-420a-8ad0-2e08a3e568ff",
        "outputId": "d323d479-2513-4919-fb9b-466f1fb40126",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ],
      "source": [
        "# Read all text.\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(text[0:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e065cd8a-f0ca-44d6-94ea-914f98a0b829",
      "metadata": {
        "id": "e065cd8a-f0ca-44d6-94ea-914f98a0b829"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0d445007-33f4-46bf-a305-fd0c15c8aaeb",
      "metadata": {
        "id": "0d445007-33f4-46bf-a305-fd0c15c8aaeb"
      },
      "source": [
        "## Build a simple next-character-prediction model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5123610b-4b8a-4850-96b4-ceba0a3933d4",
      "metadata": {
        "id": "5123610b-4b8a-4850-96b4-ceba0a3933d4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "942c94ae-8777-4bde-8d25-93fc0db16bd1",
      "metadata": {
        "id": "942c94ae-8777-4bde-8d25-93fc0db16bd1"
      },
      "source": [
        "### Tokenizer\n",
        "The tokenizer is super simple here: it's all characters that occur in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "edcda719-ec6b-4eef-bcca-0299ee3f4c3d",
      "metadata": {
        "id": "edcda719-ec6b-4eef-bcca-0299ee3f4c3d"
      },
      "outputs": [],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a character, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a99c2ced-9661-4af9-a591-87dda2378e00",
      "metadata": {
        "id": "a99c2ced-9661-4af9-a591-87dda2378e00",
        "outputId": "f1e737a2-c2aa-4ef2-ca15-7997fba95e77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
          ]
        }
      ],
      "source": [
        "print(stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0e8903b1-39c0-4df4-ab59-597a00e90fe9",
      "metadata": {
        "id": "0e8903b1-39c0-4df4-ab59-597a00e90fe9",
        "outputId": "492d9098-660c-4861-ed38-9e25eae49eba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "[[31], [54], [43], [39], [49], [6], [1], [57], [54], [43], [39], [49], [2]]\n"
          ]
        }
      ],
      "source": [
        "print(text[0:100])\n",
        "print([encode(c) for c in \"Speak, speak!\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "948228d1-9572-4796-b18c-0fc9b374c20d",
      "metadata": {
        "id": "948228d1-9572-4796-b18c-0fc9b374c20d"
      },
      "source": [
        "### Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9824e61d-abbc-4fb4-880d-487e7b5569e4",
      "metadata": {
        "id": "9824e61d-abbc-4fb4-880d-487e7b5569e4",
        "outputId": "5e0f26fb-3204-4d85-c2ce-92b43a97c77f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We'll use input 'N' to predict the next character 'a'.\n"
          ]
        }
      ],
      "source": [
        "# We want to predict the next character\n",
        "\n",
        "ix=1339\n",
        "\n",
        "# example 1.1:\n",
        "print(f\"We'll use input '{text[ix:ix+1]}' to predict the next character '{text[ix+1:ix+2]}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2f3082c8-4e79-4eab-b66d-7967231ec576",
      "metadata": {
        "id": "2f3082c8-4e79-4eab-b66d-7967231ec576",
        "outputId": "3a33754a-2d35-41e6-ae7e-ae0a4f87fdba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We'll use input 'N' to predict the next character 'a'.\n",
            "We'll use input 'Na' to predict the next character 'y'.\n",
            "We'll use input 'Nay' to predict the next character ','.\n",
            "We'll use input 'Nay,' to predict the next character ' '.\n",
            "We'll use input 'Nay, ' to predict the next character 'b'.\n",
            "We'll use input 'Nay, b' to predict the next character 'u'.\n",
            "We'll use input 'Nay, bu' to predict the next character 't'.\n",
            "We'll use input 'Nay, but' to predict the next character ' '.\n",
            "We'll use input 'Nay, but ' to predict the next character 's'.\n",
            "We'll use input 'Nay, but s' to predict the next character 'p'.\n",
            "We'll use input 'Nay, but sp' to predict the next character 'e'.\n",
            "We'll use input 'Nay, but spe' to predict the next character 'a'.\n",
            "We'll use input 'Nay, but spea' to predict the next character 'k'.\n"
          ]
        }
      ],
      "source": [
        "for j in range(13):\n",
        "    print(f\"We'll use input '{text[ix:ix+j+1]}' to predict the next character '{text[ix+j+1:ix+j+2]}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac8f15f9-3e1e-49c8-93a1-88aa28c0989c",
      "metadata": {
        "id": "ac8f15f9-3e1e-49c8-93a1-88aa28c0989c"
      },
      "source": [
        "So one example of 13 characters is actually 13 sub examples!\n",
        "\n",
        "We start with zero-length sequences because we want to make the model robust to starting from scratch.\n",
        "\n",
        "Let's implement this in a structural way so that this whole data generation thing happens automatically. We'll also add some batches because we want to make sure the GPU's are flooded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ec4813d6-7271-4ba6-a204-c05933a4b87d",
      "metadata": {
        "id": "ec4813d6-7271-4ba6-a204-c05933a4b87d"
      },
      "outputs": [],
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "block_size=13\n",
        "batch_size=8\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # note the random sampling from the original data\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0292dcd1-95bf-4b22-a2f4-b2391f33634c",
      "metadata": {
        "id": "0292dcd1-95bf-4b22-a2f4-b2391f33634c"
      },
      "source": [
        "Let's try it out and see if it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4a9b031a-0ac1-4ff3-afdb-5b1dd369451f",
      "metadata": {
        "id": "4a9b031a-0ac1-4ff3-afdb-5b1dd369451f",
        "outputId": "a948ffec-1e05-4a28-c3d7-19e6a93d2013",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----\n",
            "x vector:  tensor([ 2,  0, 20, 43, 39, 56,  1, 63, 53, 59,  1, 58, 46], device='cuda:0')\n",
            "x text:  !\n",
            "Hear you th\n",
            "y vector:  tensor([ 2,  0, 20, 43, 39, 56,  1, 63, 53, 59,  1, 58, 46], device='cuda:0')\n",
            "y text:  \n",
            "Hear you thi\n",
            "----\n",
            "x vector:  tensor([56, 12,  1, 45, 47, 60, 43,  1, 51, 43,  1, 39,  1], device='cuda:0')\n",
            "x text:  r? give me a \n",
            "y vector:  tensor([56, 12,  1, 45, 47, 60, 43,  1, 51, 43,  1, 39,  1], device='cuda:0')\n",
            "y text:  ? give me a c\n",
            "----\n",
            "x vector:  tensor([ 6,  1, 39, 50, 39, 57,  6,  0, 32, 46, 39, 52,  1], device='cuda:0')\n",
            "x text:  , alas,\n",
            "Than \n",
            "y vector:  tensor([ 6,  1, 39, 50, 39, 57,  6,  0, 32, 46, 39, 52,  1], device='cuda:0')\n",
            "y text:   alas,\n",
            "Than t\n"
          ]
        }
      ],
      "source": [
        "x,y = get_batch(\"train\")\n",
        "for idx in range(3):\n",
        "    xi=x[idx]\n",
        "    yi=y[idx]\n",
        "    print(\"----\")\n",
        "    print(f\"x vector: \", xi)\n",
        "    print(\"x text: \", \"\".join([itos[int(i)] for i in xi]))\n",
        "    print(f\"y vector: \", xi)\n",
        "    print(\"y text: \", \"\".join([itos[int(i)] for i in yi]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11ebde57-7be5-4b66-a211-5653dc4ed2ad",
      "metadata": {
        "id": "11ebde57-7be5-4b66-a211-5653dc4ed2ad"
      },
      "source": [
        "We are randomly sampling snippets of text of `block_size` long. Then, for each of those snippets we are creating multiple examples: for every character we want to predict the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d6c64de-2060-45c4-9bbc-c9893352c787",
      "metadata": {
        "id": "1d6c64de-2060-45c4-9bbc-c9893352c787"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a33ebf59-7f83-4193-833f-7601ae6e3508",
      "metadata": {
        "id": "a33ebf59-7f83-4193-833f-7601ae6e3508"
      },
      "outputs": [],
      "source": [
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecb89fca-2af9-4dda-8ea2-cee6c8f42c34",
      "metadata": {
        "id": "ecb89fca-2af9-4dda-8ea2-cee6c8f42c34"
      },
      "source": [
        "#### What does a random model do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b32bc7c9-4c59-4a82-9866-f2702d18e3fc",
      "metadata": {
        "id": "b32bc7c9-4c59-4a82-9866-f2702d18e3fc",
        "outputId": "ec5b6363-ab95-48be-9283-e94ad36ac463",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "juJu?$pc oKYa;IN3:geyH.;-HSsCKNUORQXknmgZKCaQGiSQYuSDpUzK$BjexP$Qpn?;DtZM:Z!,Tjq;CxFNMnJMA:LOcbQCifU$bbW!c LvolM3xVBNBSchqhZ&m?B;&& 3TE:qP3veZoSlNHP\n",
            "EgXHTo-ltPLrpcdUBQfmRZQbtQGidK$EsCxDB?iICgUbjRXKHlJS3HJSTnAc;VYEcccSEJRbqxEuR'WMQrNMVrbVaSd;EANQl&!ptKPDge 'bj\n",
            "XRyexXwm?SSXhuNAob\n",
            "yIWjg.oz'jB.!xUZhuxG;Xh;CDg3xD OdUA-DkGwRK!QXGwvXhrLr;emwyIYu&h'EN?-'UToUBSeQISAcV&ZxON.jAZHSN;eD'?BYaY3lnWX3y.!K!?!OCLzKidkQ:xFF$Cnmmvv!OTxX3pbPZEWuXw!kO;gpmcEhqth&vv\n",
            "?uxFNfJSLtMYuYC$UjrQ:ciSSJRpUvhw\n",
            "jBf.M;!qfIuQF:E;DlHG\n"
          ]
        }
      ],
      "source": [
        "# generate from random model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d5d757-9650-42a0-95b6-630b92c45244",
      "metadata": {
        "id": "50d5d757-9650-42a0-95b6-630b92c45244"
      },
      "source": [
        "### Add a simple way to evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9e75f529-665d-433b-9839-60ec008d0882",
      "metadata": {
        "id": "9e75f529-665d-433b-9839-60ec008d0882"
      },
      "outputs": [],
      "source": [
        "# Add some way to evaluate\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11afab5f-dad6-4570-b1dd-c00d3711fa18",
      "metadata": {
        "id": "11afab5f-dad6-4570-b1dd-c00d3711fa18"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3ce63fbd-94ce-405b-964e-23420f3169ef",
      "metadata": {
        "id": "3ce63fbd-94ce-405b-964e-23420f3169ef",
        "outputId": "58956811-7edc-4522-be26-ba91d779f6b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.5257, val loss 4.5221\n",
            "step 2000: train loss 2.4782, val loss 2.4994\n",
            "step 4000: train loss 2.4611, val loss 2.4870\n",
            "step 6000: train loss 2.4673, val loss 2.4893\n",
            "step 8000: train loss 2.4583, val loss 2.4918\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# create a PyTorch optimizer\n",
        "learning_rate = 1e-2\n",
        "max_iters = 10000\n",
        "eval_iters=500\n",
        "eval_interval=max_iters//5\n",
        "def run_training_loop():\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for iter in range(max_iters):\n",
        "\n",
        "        # every once in a while evaluate the loss on train and val sets\n",
        "        if iter % eval_interval == 0:\n",
        "            losses = estimate_loss()\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # sample a batch of data\n",
        "        xb, yb = get_batch('train')\n",
        "\n",
        "        # evaluate the loss\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "run_training_loop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "366d0183-752f-4ea7-906c-efe0a42ddeef",
      "metadata": {
        "id": "366d0183-752f-4ea7-906c-efe0a42ddeef"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "678e96b1-4789-41e1-afb3-5c54373fa618",
      "metadata": {
        "id": "678e96b1-4789-41e1-afb3-5c54373fa618"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d770648f-c8de-430b-845b-279253e59739",
      "metadata": {
        "id": "d770648f-c8de-430b-845b-279253e59739",
        "outputId": "7c24dfb2-2887-4793-9954-4bae3774f66e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MII losf win Soume teren he\n",
            "\n",
            "Bus tstinit se he me'er d, f ar, w' d.\n",
            "F yo nsit hu:\n",
            "--foshe, teit fithothe ce d itsucemy hotard matisththealoreiu\n",
            "ARIER:\n",
            "BAne ted-her?\n",
            "\n",
            "MARIny te fuis athie n mat ontisw my m dshere ulad, the\n",
            "'cildse leld t or waine ar, by mo hashilodscasuthe,\n",
            "An lliporth ick:\n",
            "\n",
            "PESh ar f l t may meen: f w t, f hes tilllatifou m; rs.\n",
            "\n",
            "SS:\n",
            "A chesoathtoof br' welke.\n",
            "CHaled pa 'sth'diof y oftr th d, ick wake s gro wbld med ar.\n",
            "\n",
            "Wh h, y t heve.\n",
            "Je peat\n",
            "The if hotangrs ase wrir pllen!\n",
            "BES\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eabb72f-05ac-4fe8-9c4d-946ea99e1bb5",
      "metadata": {
        "id": "5eabb72f-05ac-4fe8-9c4d-946ea99e1bb5"
      },
      "source": [
        "This is not really recognizable text yet, but you can tell that the model is doing something: it tries to make things that have a similar number of characters as words, sentences, punctuation, ...\n",
        "From time to time, some actual common words like \"The\", \"And\" might already appear."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "926a3d88-593a-4317-896d-49e533fb62b5",
      "metadata": {
        "id": "926a3d88-593a-4317-896d-49e533fb62b5"
      },
      "source": [
        "# Adding the first block of Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0ce17cf8-659f-4e5c-a169-fdb9fed8a58a",
      "metadata": {
        "id": "0ce17cf8-659f-4e5c-a169-fdb9fed8a58a"
      },
      "outputs": [],
      "source": [
        "head_size = 32\n",
        "n_embd = 32\n",
        "dropout=0.2\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1)  # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei * k.shape[-1]**-0.5 # keep everything in the same scale.\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) - make sure future tokens aren't accessible\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T) - scale everything nicely\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class OneHeadedModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        self.head = Head(head_size)\n",
        "\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.head(x) # (B,T,C)\n",
        "\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = OneHeadedModel()\n",
        "m = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8420d584-0998-4171-b615-93d289dfd50b",
      "metadata": {
        "id": "8420d584-0998-4171-b615-93d289dfd50b",
        "outputId": "b518ef09-4a54-443d-fddc-4e5abddf039f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2157, val loss 4.2189\n",
            "step 2000: train loss 2.5736, val loss 2.5812\n",
            "step 4000: train loss 2.4851, val loss 2.4984\n",
            "step 6000: train loss 2.4575, val loss 2.4611\n",
            "step 8000: train loss 2.4191, val loss 2.4364\n"
          ]
        }
      ],
      "source": [
        "# Same training loop as before\n",
        "\n",
        "learning_rate = 1e-3\n",
        "max_iters = 10000\n",
        "eval_interval=max_iters//5\n",
        "\n",
        "run_training_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f8ac75ac-4f4f-4757-8585-f0bb1cb9e684",
      "metadata": {
        "id": "f8ac75ac-4f4f-4757-8585-f0bb1cb9e684",
        "outputId": "487d977e-3957-4ba9-8a03-b9d5c8c2d053",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ces b s, wiltithe t By oann ndy adad tugh ry thad Lourr thot or sthough, yain veivee andd he tatham,\n",
            "is whe ptil ors bruletiloby et, lchiser\n",
            "e ak hi arykyet t cooi acou heypsow han ivngafidit wiy lhoalladearyo she asiwhe bo:\n",
            "hane han ut wisin\n",
            "Col or.\n",
            "PHea laknford dw beryathid Inkgle I itlosouart\n",
            "pod they ol;yagibolll, woncencobebe;\n",
            "If',n yot I wisimopt thor thaiful tit doi lioouso, hincefe?\n",
            "\n",
            "\n",
            "\n",
            "Whigut Ye folllll I hy ptay iuts noo ask wsas ngh t othe t habis sun ol ble coeod tho awia ckeer:\n",
            "s, c\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b759d4d8-da82-4717-b95a-1fea149f9c70",
      "metadata": {
        "id": "b759d4d8-da82-4717-b95a-1fea149f9c70"
      },
      "source": [
        "## Adding multiple heads, and finish the whole block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "067fa56c-2488-4a55-a932-07fd91229250",
      "metadata": {
        "id": "067fa56c-2488-4a55-a932-07fd91229250"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "fb9f8c45-f383-43c1-b473-f888c7edb32b",
      "metadata": {
        "id": "fb9f8c45-f383-43c1-b473-f888c7edb32b",
        "outputId": "41c94ffe-5aed-4f47-cd54-8cfb732734d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.017313 M parameters\n"
          ]
        }
      ],
      "source": [
        "n_layer=1\n",
        "n_head=4\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "11b08403-e2ab-4271-b1db-09d4bd002dd1",
      "metadata": {
        "id": "11b08403-e2ab-4271-b1db-09d4bd002dd1",
        "outputId": "862e3016-3c8e-453e-b840-1e2fc27ed1ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1594, val loss 4.1577\n",
            "step 2000: train loss 2.2754, val loss 2.2915\n",
            "step 4000: train loss 2.1747, val loss 2.2305\n",
            "step 6000: train loss 2.1274, val loss 2.1935\n",
            "step 8000: train loss 2.1062, val loss 2.1689\n"
          ]
        }
      ],
      "source": [
        "run_training_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2f0556cf-3b1b-4041-948e-b74bd3ce7399",
      "metadata": {
        "id": "2f0556cf-3b1b-4041-948e-b74bd3ce7399",
        "outputId": "50a48b40-fd57-4af4-f06a-c7e224dc3bb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Must for alll risty\n",
            "If flayer;\n",
            "the and, fae,\n",
            "Heas se willlery.\n",
            "\n",
            "WDY:\n",
            "Whus thery sin or she the wheae ender fhither? \n",
            "\n",
            "B now loved the reoowsior rsw tublotens, of lAlsse-aplick\n",
            "COMIAR:\n",
            "Which oul\n",
            "thinf roulve wort not pror baim this chient to thrane the evir my towerche thy reled-mohherssilen wof the affy he cal inke plats?\n",
            "\n",
            "Paciund ple the ther maep'd plold\n",
            "ther thim,\n",
            "My tho ad aning the whath Gasto Of isce oviand and ill angrte hell, that we-sevar mirgting, hinirereto t musp.\n",
            "\n",
            "HEOLIIXEO:\n",
            "Herry w\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c50c10e-b462-4095-996a-4f0921853f28",
      "metadata": {
        "id": "4c50c10e-b462-4095-996a-4f0921853f28"
      },
      "source": [
        "## Just scale up!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5e45df4c-2a1d-417e-a12f-624ace3a490a",
      "metadata": {
        "id": "5e45df4c-2a1d-417e-a12f-624ace3a490a",
        "outputId": "45255bef-c515-4a64-a6b8-2d0f229acf69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length (here: number of characters) for predictions?\n",
        "learning_rate = 3e-4\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "max_iters = 3000\n",
        "eval_interval=max_iters//10\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "11229dfc-3a70-4fd5-971d-481d1282ecfb",
      "metadata": {
        "id": "11229dfc-3a70-4fd5-971d-481d1282ecfb",
        "outputId": "9026d0ad-f948-488d-d8a3-cfa40731c14d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2256, val loss 4.2257\n",
            "step 300: train loss 2.2304, val loss 2.2835\n",
            "step 600: train loss 1.6495, val loss 1.8231\n",
            "step 900: train loss 1.4495, val loss 1.6590\n",
            "step 1200: train loss 1.3485, val loss 1.5830\n",
            "step 1500: train loss 1.2803, val loss 1.5405\n",
            "step 1800: train loss 1.2191, val loss 1.5082\n",
            "step 2100: train loss 1.1823, val loss 1.4997\n",
            "step 2400: train loss 1.1449, val loss 1.4863\n",
            "step 2700: train loss 1.1099, val loss 1.4802\n"
          ]
        }
      ],
      "source": [
        "if True: # Re-train?\n",
        "    # Takes about 15 min on V100.\n",
        "    run_training_loop()\n",
        "    torch.save(m.state_dict(), 'shakespeare_gpt.pth')\n",
        "# step 0: train loss 4.3823, val loss 4.3756\n",
        "# step 500: train loss 1.7285, val loss 1.8738\n",
        "# step 1000: train loss 1.4017, val loss 1.6253\n",
        "# step 1500: train loss 1.2790, val loss 1.5399\n",
        "# step 2000: train loss 1.1935, val loss 1.5008\n",
        "# step 2500: train loss 1.1329, val loss 1.4952\n",
        "# step 3000: train loss 1.0720, val loss 1.5008\n",
        "\n",
        "else:\n",
        "    model = GPTLanguageModel()  # Replace with your model's class\n",
        "    model.load_state_dict(torch.load('shakespeare_gpt.pth', map_location=torch.device(device)))\n",
        "    model.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "442c395e-b990-4da8-a588-7d2d61db7ca1",
      "metadata": {
        "id": "442c395e-b990-4da8-a588-7d2d61db7ca1",
        "outputId": "47a85d6d-e9a1-4d88-9892-8ab643dcb97a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Not to your maid, Henry Perdil,\n",
            "Then hollour and respect to be the fault,\n",
            "Plantagenet. To let me fle Marcius\n",
            "Method Rome, do not the nuts along live\n",
            "The oath of that rebel' my daughter, yet all carrion\n",
            "That Wars and Trence ta's barry;\n",
            "And whither down to sproud Salisbury\n",
            "With the offen whose nursed was govern'd,\n",
            "And than more adm, lat thy ruin:\n",
            "Give that thou wiltst not rule me--I leave.\n",
            "Witness thy sighs?\n",
            "\n",
            "RATCLIFF:\n",
            "Richard, Paus Margaretha is enemy,\n",
            "Great hath left my child, aside;\n",
            "Or a take i\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22b8023d-036b-4bd6-8df0-9b2e07c72514",
      "metadata": {
        "id": "22b8023d-036b-4bd6-8df0-9b2e07c72514"
      },
      "source": [
        "## Embeddings\n",
        "Now, we'll dive into the model specifics and see a little trick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "fa54735e-ef13-45cd-96a7-669ff3b3ef5a",
      "metadata": {
        "id": "fa54735e-ef13-45cd-96a7-669ff3b3ef5a"
      },
      "outputs": [],
      "source": [
        "# Tokenize the words (convert each character to its token ID)\n",
        "words= [\"ROMEO\", \"JULIET\", \"ELIZABETH\",\"queen\", \"love\"]\n",
        "tokenized_words = [[stoi[char] for char in word] for word in words]\n",
        "\n",
        "# Convert to tensor and pad sequences for equal length\n",
        "max_len = max(len(t) for t in tokenized_words)\n",
        "padded_tokens = [t + [0] * (max_len - len(t)) for t in tokenized_words]\n",
        "input_ids = torch.tensor(padded_tokens).to(device)\n",
        "\n",
        "# Forward pass\n",
        "outputs = m(input_ids)\n",
        "\n",
        "# Aggregating character embeddings to get word embeddings\n",
        "# Here, using simple averaging\n",
        "words_emb = outputs[0].mean(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d5534cc5-1961-4465-a6a4-2ea3768a996e",
      "metadata": {
        "id": "d5534cc5-1961-4465-a6a4-2ea3768a996e",
        "outputId": "48302c32-d9f1-44c6-b035-060cff301163",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JULIET-ROMEO: 0.94\n",
            "ELIZABETH-ROMEO: 0.80\n",
            "ELIZABETH-JULIET: 0.82\n",
            "queen-ROMEO: 0.43\n",
            "queen-JULIET: 0.49\n",
            "queen-ELIZABETH: 0.21\n",
            "love-ROMEO: 0.64\n",
            "love-JULIET: 0.67\n",
            "love-ELIZABETH: 0.36\n",
            "love-queen: 0.91\n"
          ]
        }
      ],
      "source": [
        "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "for i in range(len(words)):\n",
        "    for j in range(i):\n",
        "        sim = cos(words_emb[i], words_emb[j])\n",
        "        print(f\"{words[i]}-{words[j]}: {sim:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "5b1ad043-0007-43cd-a73e-48dc557a7f58",
      "metadata": {
        "id": "5b1ad043-0007-43cd-a73e-48dc557a7f58"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d3a72399-3b92-4f4a-8bd7-7d8773001b33",
      "metadata": {
        "id": "d3a72399-3b92-4f4a-8bd7-7d8773001b33"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}