{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43131da9-de8d-409b-8535-a1c2fafd7440",
   "metadata": {
    "id": "43131da9-de8d-409b-8535-a1c2fafd7440"
   },
   "source": [
    "# Let's build a language model\n",
    "This lecture is inspired by and uses some code from Andrej Karpathy's excellent lecture series on NLP.\n",
    "The original Transformers lecture can be watched on [Youtube](https://www.youtube.com/watch?v=kCc8FmEb1nY). I encourage you to do so: having two people explain the same usually benefits understanding a lot.\n",
    "\n",
    "## Further reading\n",
    "1. To broaden your understanding even further, you can also study an alternative implementation doing something similar, noting the differences. For example [GPT in 60 lines of code](https://jaykmody.com/blog/gpt-from-scratch/).\n",
    "2. The original paper introducing the Transformer architecture: [Attention is all you need](https://arxiv.org/abs/1706.03762).\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff12b39f-4545-4ada-9924-486cdf827872",
   "metadata": {
    "id": "ff12b39f-4545-4ada-9924-486cdf827872"
   },
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048dcacb-bb28-4fdc-9646-8647fde657c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "048dcacb-bb28-4fdc-9646-8647fde657c1",
    "outputId": "30b85b18-083b-49c4-b613-f9b202d87696"
   },
   "outputs": [],
   "source": [
    "!wget -O DATA/shakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt \n",
    "!cat DATA/shakespeare.txt | tr ' ' '\\n' | sort | uniq -c | sort -nr | head -n50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ce845-9059-420a-8ad0-2e08a3e568ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c7ce845-9059-420a-8ad0-2e08a3e568ff",
    "outputId": "d323d479-2513-4919-fb9b-466f1fb40126"
   },
   "outputs": [],
   "source": [
    "# Read all text.\n",
    "with open('DATA/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065cd8a-f0ca-44d6-94ea-914f98a0b829",
   "metadata": {
    "id": "e065cd8a-f0ca-44d6-94ea-914f98a0b829"
   },
   "outputs": [],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d445007-33f4-46bf-a305-fd0c15c8aaeb",
   "metadata": {
    "id": "0d445007-33f4-46bf-a305-fd0c15c8aaeb"
   },
   "source": [
    "## Build a simple next-character-prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123610b-4b8a-4850-96b4-ceba0a3933d4",
   "metadata": {
    "id": "5123610b-4b8a-4850-96b4-ceba0a3933d4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c94ae-8777-4bde-8d25-93fc0db16bd1",
   "metadata": {
    "id": "942c94ae-8777-4bde-8d25-93fc0db16bd1"
   },
   "source": [
    "### Tokenizer\n",
    "The tokenizer is super simple here: it's all characters that occur in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1628f1-3434-4d67-af21-30cf55592044",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30152267-4497-4bc8-89d2-b67c0f02ca1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea87ca-9740-4cc8-a11c-90b47dcee08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa7fc7f-a492-44e9-971b-57a517fe2d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcda719-ec6b-4eef-bcca-0299ee3f4c3d",
   "metadata": {
    "id": "edcda719-ec6b-4eef-bcca-0299ee3f4c3d"
   },
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a character, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99c2ced-9661-4af9-a591-87dda2378e00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a99c2ced-9661-4af9-a591-87dda2378e00",
    "outputId": "f1e737a2-c2aa-4ef2-ca15-7997fba95e77"
   },
   "outputs": [],
   "source": [
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8903b1-39c0-4df4-ab59-597a00e90fe9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e8903b1-39c0-4df4-ab59-597a00e90fe9",
    "outputId": "492d9098-660c-4861-ed38-9e25eae49eba"
   },
   "outputs": [],
   "source": [
    "# print(text[0:100])\n",
    "print([encode(c) for c in \"Speak, speak!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948228d1-9572-4796-b18c-0fc9b374c20d",
   "metadata": {
    "id": "948228d1-9572-4796-b18c-0fc9b374c20d"
   },
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824e61d-abbc-4fb4-880d-487e7b5569e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9824e61d-abbc-4fb4-880d-487e7b5569e4",
    "outputId": "5e0f26fb-3204-4d85-c2ce-92b43a97c77f"
   },
   "outputs": [],
   "source": [
    "# We want to predict the next character\n",
    "\n",
    "random_idx=15009\n",
    "\n",
    "# example 1.1:\n",
    "print(f\"We'll use input '{text[random_idx:random_idx+1]}' to predict the next character '{text[random_idx+1:random_idx+2]}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3082c8-4e79-4eab-b66d-7967231ec576",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f3082c8-4e79-4eab-b66d-7967231ec576",
    "outputId": "3a33754a-2d35-41e6-ae7e-ae0a4f87fdba"
   },
   "outputs": [],
   "source": [
    "for j in range(13):\n",
    "    print(f\"We'll use input '{text[random_idx:random_idx+j+1]}' to predict the next character '{text[random_idx+j+1:random_idx+j+2]}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eaf4b5-4edf-453b-8832-7028aac5c47d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac8f15f9-3e1e-49c8-93a1-88aa28c0989c",
   "metadata": {
    "id": "ac8f15f9-3e1e-49c8-93a1-88aa28c0989c"
   },
   "source": [
    "So one example of 13 characters is actually 13 sub examples!\n",
    "\n",
    "We start with zero-length sequences because we want to make the model robust to starting from scratch.\n",
    "\n",
    "Let's implement this in a structural way so that this whole data generation thing happens automatically. We'll also add some batches because we want to make sure the GPU's are flooded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a7af5-01ed-4a3a-ac90-7e5aa8912036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac4c57-32c9-41dc-b59d-161093458465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "block_size=13 # sometimes called context length, sequence length, ... also sometimes called the TIME dimension\n",
    "batch_size=8 # independent sequences\n",
    "\n",
    "random_idx = 15014\n",
    "for j in range(13):\n",
    "    print(f\"We'll use input '{train_data[random_idx:random_idx+j+1]}' to predict the next character '{train_data[random_idx+j+1:random_idx+j+2]}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e333d-d8e7-45c3-86b3-dac1afa40a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(len(data) - block_size, (batch_size,)) # note the random sampling from the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4813d6-7271-4ba6-a204-c05933a4b87d",
   "metadata": {
    "id": "ec4813d6-7271-4ba6-a204-c05933a4b87d"
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # note the random sampling from the original data\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0292dcd1-95bf-4b22-a2f4-b2391f33634c",
   "metadata": {
    "id": "0292dcd1-95bf-4b22-a2f4-b2391f33634c"
   },
   "source": [
    "Let's try it out and see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b031a-0ac1-4ff3-afdb-5b1dd369451f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a9b031a-0ac1-4ff3-afdb-5b1dd369451f",
    "outputId": "a948ffec-1e05-4a28-c3d7-19e6a93d2013"
   },
   "outputs": [],
   "source": [
    "x,y = get_batch(\"train\")\n",
    "for idx in range(3):\n",
    "    xi=x[idx]\n",
    "    yi=y[idx]\n",
    "    print(\"----\")\n",
    "    print(f\"x vector: \", xi)\n",
    "    print(\"x text: \", \"\".join([itos[int(i)] for i in xi]))\n",
    "    print(f\"y vector: \", xi)\n",
    "    print(\"y text: \", \"\".join([itos[int(i)] for i in yi]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ebde57-7be5-4b66-a211-5653dc4ed2ad",
   "metadata": {
    "id": "11ebde57-7be5-4b66-a211-5653dc4ed2ad"
   },
   "source": [
    "We are randomly sampling snippets of text of `block_size` long. Then, for each of those snippets we are creating multiple examples: for every character we want to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f22970-9590-4a86-9cb5-eb1407011913",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(y)\n",
    "for i in range(13):\n",
    "    print(f\"## Sub-example {i}\")\n",
    "    # print(x.shape) # so total nr of examples is 8*13\n",
    "    print(\"x:\", x[:,:i+1])\n",
    "    # print(y.shape)\n",
    "    print(\"y:\", y[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6c64de-2060-45c4-9bbc-c9893352c787",
   "metadata": {
    "id": "1d6c64de-2060-45c4-9bbc-c9893352c787"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ebf59-7f83-4193-833f-7601ae6e3508",
   "metadata": {
    "id": "a33ebf59-7f83-4193-833f-7601ae6e3508"
   },
   "outputs": [],
   "source": [
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"Bi-gram model: predict characters based on previous character only.\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (Batch, Time, Channel)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # get matrices in right format for F.cross_entropy\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        total_sequence = idx\n",
    "        idx_next = idx\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx_next)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits[0], dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            total_sequence = torch.cat((total_sequence, idx_next), dim=1) # (B, T+1)    \n",
    "        return total_sequence\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cbb075-0ad2-4b37-838d-8e6a27ead1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = m(x, y)\n",
    "logits\n",
    "# logits.shape # One logit for every character in vocab, for every B*T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb89fca-2af9-4dda-8ea2-cee6c8f42c34",
   "metadata": {
    "id": "ecb89fca-2af9-4dda-8ea2-cee6c8f42c34"
   },
   "source": [
    "#### What does a random model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32bc7c9-4c59-4a82-9866-f2702d18e3fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b32bc7c9-4c59-4a82-9866-f2702d18e3fc",
    "outputId": "ec5b6363-ab95-48be-9283-e94ad36ac463"
   },
   "outputs": [],
   "source": [
    "# generate from random model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) #start with character 0 which is newline\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d5d757-9650-42a0-95b6-630b92c45244",
   "metadata": {
    "id": "50d5d757-9650-42a0-95b6-630b92c45244"
   },
   "source": [
    "### Add a simple way to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75f529-665d-433b-9839-60ec008d0882",
   "metadata": {
    "id": "9e75f529-665d-433b-9839-60ec008d0882"
   },
   "outputs": [],
   "source": [
    "# Add some way to evaluate\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11afab5f-dad6-4570-b1dd-c00d3711fa18",
   "metadata": {
    "id": "11afab5f-dad6-4570-b1dd-c00d3711fa18"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce63fbd-94ce-405b-964e-23420f3169ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ce63fbd-94ce-405b-964e-23420f3169ef",
    "outputId": "58956811-7edc-4522-be26-ba91d779f6b4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# create a PyTorch optimizer\n",
    "learning_rate = 1e-2\n",
    "max_iters = 10000\n",
    "eval_iters=500\n",
    "eval_interval=max_iters//5\n",
    "def run_training_loop():\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    for iter in range(max_iters):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "run_training_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d0183-752f-4ea7-906c-efe0a42ddeef",
   "metadata": {
    "id": "366d0183-752f-4ea7-906c-efe0a42ddeef"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "678e96b1-4789-41e1-afb3-5c54373fa618",
   "metadata": {
    "id": "678e96b1-4789-41e1-afb3-5c54373fa618"
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770648f-c8de-430b-845b-279253e59739",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d770648f-c8de-430b-845b-279253e59739",
    "outputId": "7c24dfb2-2887-4793-9954-4bae3774f66e"
   },
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabb72f-05ac-4fe8-9c4d-946ea99e1bb5",
   "metadata": {
    "id": "5eabb72f-05ac-4fe8-9c4d-946ea99e1bb5"
   },
   "source": [
    "This is not really recognizable text yet, but you can tell that the model is doing something: it tries to make things that have a similar number of characters as words, sentences, punctuation, ...\n",
    "From time to time, some actual common words like \"The\", \"And\" might already appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a3d88-593a-4317-896d-49e533fb62b5",
   "metadata": {
    "id": "926a3d88-593a-4317-896d-49e533fb62b5"
   },
   "source": [
    "# Adding the first block of Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce17cf8-659f-4e5c-a169-fdb9fed8a58a",
   "metadata": {
    "id": "0ce17cf8-659f-4e5c-a169-fdb9fed8a58a"
   },
   "outputs": [],
   "source": [
    "\n",
    "n_embd = 32\n",
    "head_size = n_embd // 1\n",
    "\n",
    "dropout=0.2\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "\n",
    "        \n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention takes a sequence, adds values of other tokens in the same sequence and outputs a new sequence of dim hs.\n",
    "        # \n",
    "        # input of size (batch, time-step = sequence length = context length = block_size, channels = n_embed)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,head_size)\n",
    "        q = self.query(x) # (B,T,head_size)\n",
    "        # compute attention scores (\"affinities\")\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1)  # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        \n",
    "        wei = wei * k.shape[-1]**-0.5 # keep everything in the same scale.\n",
    "        \n",
    "        # wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) - make sure future tokens aren't accessible, only for decoder blocks. For encoder blocks: skip!\n",
    "        \n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T) - scale everything nicely to sum per row =1\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs) is the output of the head\n",
    "        return out\n",
    "\n",
    "class OneHeadedModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # maps characters to internal state per character\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # maps positions of characters to internal state per index\n",
    "        \n",
    "        self.head = Head(head_size)\n",
    "\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "    \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,n_embd): 0,1,2,3,... fixed sequence\n",
    "        x = tok_emb + pos_emb # (B,T,n_embd)\n",
    "        x = self.head(x) # (B,T,n_embd)\n",
    "\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\" Slightly different since we now have history\"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = OneHeadedModel()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a0267e-6239-42f6-a426-d416854511e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420d584-0998-4171-b615-93d289dfd50b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8420d584-0998-4171-b615-93d289dfd50b",
    "outputId": "b518ef09-4a54-443d-fddc-4e5abddf039f"
   },
   "outputs": [],
   "source": [
    "# Same training loop as before\n",
    "\n",
    "learning_rate = 1e-3\n",
    "max_iters = 10000\n",
    "eval_interval=max_iters//5\n",
    "\n",
    "run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac75ac-4f4f-4757-8585-f0bb1cb9e684",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8ac75ac-4f4f-4757-8585-f0bb1cb9e684",
    "outputId": "487d977e-3957-4ba9-8a03-b9d5c8c2d053"
   },
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b759d4d8-da82-4717-b95a-1fea149f9c70",
   "metadata": {
    "id": "b759d4d8-da82-4717-b95a-1fea149f9c70"
   },
   "source": [
    "## Adding multiple heads, and finish the whole block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067fa56c-2488-4a55-a932-07fd91229250",
   "metadata": {
    "id": "067fa56c-2488-4a55-a932-07fd91229250"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity. This is applied per token separately.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size) # communication\n",
    "        self.ffwd = FeedFoward(n_embd) # computation\n",
    "        self.ln1 = nn.LayerNorm(n_embd) # normalization keeping things nicely unit gaussian (at init) per token\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # note the `x +`: that's the residual connection for combating vanishing gradients\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f8c45-f383-43c1-b473-f888c7edb32b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb9f8c45-f383-43c1-b473-f888c7edb32b",
    "outputId": "41c94ffe-5aed-4f47-cd54-8cfb732734d8"
   },
   "outputs": [],
   "source": [
    "n_layer=1\n",
    "n_head=4\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b08403-e2ab-4271-b1db-09d4bd002dd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11b08403-e2ab-4271-b1db-09d4bd002dd1",
    "outputId": "862e3016-3c8e-453e-b840-1e2fc27ed1ae"
   },
   "outputs": [],
   "source": [
    "run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0556cf-3b1b-4041-948e-b74bd3ce7399",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f0556cf-3b1b-4041-948e-b74bd3ce7399",
    "outputId": "50a48b40-fd57-4af4-f06a-c7e224dc3bb7"
   },
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50c10e-b462-4095-996a-4f0921853f28",
   "metadata": {
    "id": "4c50c10e-b462-4095-996a-4f0921853f28"
   },
   "source": [
    "## Just scale up!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45df4c-2a1d-417e-a12f-624ace3a490a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e45df4c-2a1d-417e-a12f-624ace3a490a",
    "outputId": "45255bef-c515-4a64-a6b8-2d0f229acf69"
   },
   "outputs": [],
   "source": [
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length (here: number of characters) for predictions?\n",
    "learning_rate = 3e-4\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "max_iters = 3000\n",
    "eval_interval=max_iters//10\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11229dfc-3a70-4fd5-971d-481d1282ecfb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11229dfc-3a70-4fd5-971d-481d1282ecfb",
    "outputId": "9026d0ad-f948-488d-d8a3-cfa40731c14d"
   },
   "outputs": [],
   "source": [
    "if False: # Re-train?\n",
    "    # Takes about 15 min on V100.\n",
    "    run_training_loop()\n",
    "    torch.save(m.state_dict(), 'shakespeare_gpt.pth')\n",
    "# step 0: train loss 4.3823, val loss 4.3756\n",
    "# step 500: train loss 1.7285, val loss 1.8738\n",
    "# step 1000: train loss 1.4017, val loss 1.6253\n",
    "# step 1500: train loss 1.2790, val loss 1.5399\n",
    "# step 2000: train loss 1.1935, val loss 1.5008\n",
    "# step 2500: train loss 1.1329, val loss 1.4952\n",
    "# step 3000: train loss 1.0720, val loss 1.5008\n",
    "\n",
    "else:\n",
    "    model = GPTLanguageModel()  # Replace with your model's class\n",
    "    model.load_state_dict(torch.load('shakespeare_gpt.pth', map_location=torch.device(device)))\n",
    "    model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c395e-b990-4da8-a588-7d2d61db7ca1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "442c395e-b990-4da8-a588-7d2d61db7ca1",
    "outputId": "47a85d6d-e9a1-4d88-9892-8ab643dcb97a"
   },
   "outputs": [],
   "source": [
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b8023d-036b-4bd6-8df0-9b2e07c72514",
   "metadata": {
    "id": "22b8023d-036b-4bd6-8df0-9b2e07c72514"
   },
   "source": [
    "## Embeddings\n",
    "Now, we'll dive into the model specifics and see a little trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa54735e-ef13-45cd-96a7-669ff3b3ef5a",
   "metadata": {
    "id": "fa54735e-ef13-45cd-96a7-669ff3b3ef5a"
   },
   "outputs": [],
   "source": [
    "# Tokenize the words (convert each character to its token ID)\n",
    "words= [\"ROMEO\", \"JULIET\", \"ELIZABETH\",\"queen\", \"love\"]\n",
    "tokenized_words = [[stoi[char] for char in word] for word in words]\n",
    "\n",
    "# Convert to tensor and pad sequences for equal length\n",
    "max_len = max(len(t) for t in tokenized_words)\n",
    "padded_tokens = [t + [0] * (max_len - len(t)) for t in tokenized_words]\n",
    "input_ids = torch.tensor(padded_tokens).to(device)\n",
    "\n",
    "# Forward pass\n",
    "outputs = m(input_ids)\n",
    "\n",
    "# Aggregating character embeddings to get word embeddings\n",
    "# Here, using simple averaging\n",
    "words_emb = outputs[0].mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ccc843-ce2e-4eb0-8170-aa11d76764c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.lm_head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5534cc5-1961-4465-a6a4-2ea3768a996e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5534cc5-1961-4465-a6a4-2ea3768a996e",
    "outputId": "48302c32-d9f1-44c6-b035-060cff301163"
   },
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "for i in range(len(words)):\n",
    "    for j in range(i):\n",
    "        sim = cos(words_emb[i], words_emb[j])\n",
    "        print(f\"{words[i]}-{words[j]}: {sim:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ad043-0007-43cd-a73e-48dc557a7f58",
   "metadata": {
    "id": "5b1ad043-0007-43cd-a73e-48dc557a7f58"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a72399-3b92-4f4a-8bd7-7d8773001b33",
   "metadata": {
    "id": "d3a72399-3b92-4f4a-8bd7-7d8773001b33"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
