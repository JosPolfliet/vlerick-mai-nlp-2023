{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43131da9-de8d-409b-8535-a1c2fafd7440",
   "metadata": {},
   "source": [
    "# Let's build a language model\n",
    "This lecture is inspired by and uses some code from Andrej Karpathy's excellent lecture series on NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff12b39f-4545-4ada-9924-486cdf827872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "048dcacb-bb28-4fdc-9646-8647fde657c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# !cat input.txt | tr ' ' '\\n' | sort | uniq -c | sort -nr | head -n500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c7ce845-9059-420a-8ad0-2e08a3e568ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# Read all text.\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065cd8a-f0ca-44d6-94ea-914f98a0b829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d445007-33f4-46bf-a305-fd0c15c8aaeb",
   "metadata": {},
   "source": [
    "## Build a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5123610b-4b8a-4850-96b4-ceba0a3933d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c94ae-8777-4bde-8d25-93fc0db16bd1",
   "metadata": {},
   "source": [
    "### Tokenizer \n",
    "The tokenizer is super simple here: it's all characters that occur in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edcda719-ec6b-4eef-bcca-0299ee3f4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a character, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a99c2ced-9661-4af9-a591-87dda2378e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e8903b1-39c0-4df4-ab59-597a00e90fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "[[31], [54], [43], [39], [49], [6], [1], [57], [54], [43], [39], [49], [2]]\n"
     ]
    }
   ],
   "source": [
    "print(text[0:100])\n",
    "print([encode(c) for c in \"Speak, speak!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948228d1-9572-4796-b18c-0fc9b374c20d",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9824e61d-abbc-4fb4-880d-487e7b5569e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'll use input 'N' to predict the next character 'a'.\n"
     ]
    }
   ],
   "source": [
    "# We want to predict the next character\n",
    "\n",
    "ix=1339\n",
    "\n",
    "# example 1.1:\n",
    "print(f\"We'll use input '{text[ix:ix+1]}' to predict the next character '{text[ix+1:ix+2]}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2f3082c8-4e79-4eab-b66d-7967231ec576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'll use input 'N' to predict the next character 'a'.\n",
      "We'll use input 'Na' to predict the next character 'y'.\n",
      "We'll use input 'Nay' to predict the next character ','.\n",
      "We'll use input 'Nay,' to predict the next character ' '.\n",
      "We'll use input 'Nay, ' to predict the next character 'b'.\n",
      "We'll use input 'Nay, b' to predict the next character 'u'.\n",
      "We'll use input 'Nay, bu' to predict the next character 't'.\n",
      "We'll use input 'Nay, but' to predict the next character ' '.\n",
      "We'll use input 'Nay, but ' to predict the next character 's'.\n",
      "We'll use input 'Nay, but s' to predict the next character 'p'.\n",
      "We'll use input 'Nay, but sp' to predict the next character 'e'.\n",
      "We'll use input 'Nay, but spe' to predict the next character 'a'.\n",
      "We'll use input 'Nay, but spea' to predict the next character 'k'.\n"
     ]
    }
   ],
   "source": [
    "for j in range(13):\n",
    "    print(f\"We'll use input '{text[ix:ix+j+1]}' to predict the next character '{text[ix+j+1:ix+j+2]}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8f15f9-3e1e-49c8-93a1-88aa28c0989c",
   "metadata": {},
   "source": [
    "So one example of 13 characters is actually 13 sub examples!\n",
    "\n",
    "We start with zero-length sequences because we want to make the model robust to starting from scratch. \n",
    "\n",
    "Let's implement this in a structural way so that this whole data generation thing happens automatically. We'll also add some batches because we want to make sure the GPU's are flooded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec4813d6-7271-4ba6-a204-c05933a4b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "block_size=13\n",
    "batch_size=8\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0292dcd1-95bf-4b22-a2f4-b2391f33634c",
   "metadata": {},
   "source": [
    "Let's try it out and see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a9b031a-0ac1-4ff3-afdb-5b1dd369451f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "x vector:  tensor([52, 39, 63,  6,  1, 58, 46, 43, 56, 43,  5, 57,  1])\n",
      "x text:  nay, there's \n",
      "y vector:  tensor([52, 39, 63,  6,  1, 58, 46, 43, 56, 43,  5, 57,  1])\n",
      "y text:  ay, there's c\n",
      "----\n",
      "x vector:  tensor([38, 17, 24, 10,  0, 13, 54, 54, 56, 43, 46, 43, 52])\n",
      "x text:  ZEL:\n",
      "Apprehen\n",
      "y vector:  tensor([38, 17, 24, 10,  0, 13, 54, 54, 56, 43, 46, 43, 52])\n",
      "y text:  EL:\n",
      "Apprehend\n",
      "----\n",
      "x vector:  tensor([39, 58, 58, 43, 56, 57,  1, 53, 44,  1, 45, 56, 43])\n",
      "x text:  atters of gre\n",
      "y vector:  tensor([39, 58, 58, 43, 56, 57,  1, 53, 44,  1, 45, 56, 43])\n",
      "y text:  tters of grea\n"
     ]
    }
   ],
   "source": [
    "x,y = get_batch(\"train\")\n",
    "for idx in range(3):\n",
    "    xi=x[idx]\n",
    "    yi=y[idx]\n",
    "    print(\"----\")\n",
    "    print(f\"x vector: \", xi)\n",
    "    print(\"x text: \", \"\".join([itos[int(i)] for i in xi]))\n",
    "    print(f\"y vector: \", xi)\n",
    "    print(\"y text: \", \"\".join([itos[int(i)] for i in yi]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ebde57-7be5-4b66-a211-5653dc4ed2ad",
   "metadata": {},
   "source": [
    "We are randomly sampling snippets of text of `block_size` long. Then, for each of those snippets we are creating multiple examples: for every character we want to predict the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6c64de-2060-45c4-9bbc-c9893352c787",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a33ebf59-7f83-4193-833f-7601ae6e3508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb89fca-2af9-4dda-8ea2-cee6c8f42c34",
   "metadata": {},
   "source": [
    "#### What does a random model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32bc7c9-4c59-4a82-9866-f2702d18e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from random model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d5d757-9650-42a0-95b6-630b92c45244",
   "metadata": {},
   "source": [
    "### Add a simple way to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9e75f529-665d-433b-9839-60ec008d0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some way to evaluate\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11afab5f-dad6-4570-b1dd-c00d3711fa18",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3ce63fbd-94ce-405b-964e-23420f3169ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1700, val loss 4.1770\n",
      "step 2000: train loss 2.3378, val loss 2.4140\n",
      "step 4000: train loss 2.2917, val loss 2.3161\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mrun_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[100], line 19\u001b[0m, in \u001b[0;36mrun_training_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# evaluate the loss\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[99], line 42\u001b[0m, in \u001b[0;36mGPTLanguageModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     40\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(B\u001b[38;5;241m*\u001b[39mT, C)\n\u001b[1;32m     41\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mview(B\u001b[38;5;241m*\u001b[39mT)\n\u001b[0;32m---> 42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# create a PyTorch optimizer\n",
    "learning_rate = 1e-2\n",
    "max_iters = 10000\n",
    "eval_iters=500\n",
    "eval_interval=max_iters//5\n",
    "def run_training_loop():\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for iter in range(max_iters):\n",
    "    \n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "    \n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "run_training_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d0183-752f-4ea7-906c-efe0a42ddeef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "678e96b1-4789-41e1-afb3-5c54373fa618",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d770648f-c8de-430b-845b-279253e59739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHig h.\n",
      "pind citiesthel mexey pp marechefouisthad? t avend ie fey he foftotey fiou mpome m Iff I thid.\n",
      "At ve,\n",
      "ROFRMIVIfatces gadind,\n",
      "\n",
      "TIVI scksule t wds IFFine my.\n",
      "T:\n",
      "Ar.\n",
      "Whe men.\n",
      "Touriofour: tsis prtha he thetiody winy preit KE:\n",
      "S:\n",
      "\n",
      "Gr I'st cas!\n",
      "PENundwan inou LAnd n ge cout tid wnot---\n",
      "YOFofr:\n",
      "TE d whouspr, nctysomis urr s sof t, I g; yom'bere:\n",
      "ETEShrsajuthe, be veedomemy NO:\n",
      "G t ce ckerid wan. maspt gal trol beruren m oshor stour rdwn, pluthea IORKndl dir! I sero ime hithind IInor, d wngreat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabb72f-05ac-4fe8-9c4d-946ea99e1bb5",
   "metadata": {},
   "source": [
    "This is not really recognizable text yet, but you can tell that the model is doing something: it tries to make things that have a similar number of characters as words, sentences, punctuation, ... \n",
    "From time to time, some actual common words like \"The\", \"And\" might already appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a3d88-593a-4317-896d-49e533fb62b5",
   "metadata": {},
   "source": [
    "# Adding the first block of Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0ce17cf8-659f-4e5c-a169-fdb9fed8a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = 32\n",
    "n_embd = 32\n",
    "dropout=0.2\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1)  # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei * k.shape[-1]**-0.5 # keep everything in the same scale.\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) - make sure future tokens aren't accessible\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T) - scale everything nicely\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class OneHeadedModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        self.head = Head(head_size)\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.head(x) # (B,T,C)\n",
    "        \n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = OneHeadedModel()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8420d584-0998-4171-b615-93d289dfd50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.4800, val loss 2.4443\n",
      "step 2000: train loss 2.3453, val loss 2.3679\n",
      "step 4000: train loss 2.3697, val loss 2.4171\n",
      "step 6000: train loss 2.3529, val loss 2.3627\n",
      "step 8000: train loss 2.2927, val loss 2.3442\n"
     ]
    }
   ],
   "source": [
    "# Same training loop as before\n",
    "\n",
    "learning_rate = 1e-3\n",
    "max_iters = 10000\n",
    "eval_interval=max_iters//5\n",
    "\n",
    "run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f8ac75ac-4f4f-4757-8585-f0bb1cb9e684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ", ha ir smuos, lr uf hin cther e el itle as: he atan yan whith se, m, haiidor ste flo int:\n",
      "IN ctos wrld p amgprid ndon ndyC:\n",
      "Bond nat,hei sf sl:\n",
      "Wo, o'n. fout h ha ste, yechis t,\n",
      "\n",
      "Whal'illem;\n",
      "Mouty ayllll ist nclanliof hafr din ing tle achontshil sh'srar miwrince, ' f owunt le Jui\n",
      "eus loutthilor ca wist thehorif li,inev'ind bne,-en----soren r\n",
      "mawint.\n",
      "I dank lindw\n",
      "Whil oul shil re.\n",
      "Weve.\n",
      "pst sint t:\n",
      "beooe ars se fikbit 'che Ms'har.y Cre iamawon' tt bewry'd wirandceaveter,\n",
      "I h Mill rinta whath po \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b759d4d8-da82-4717-b95a-1fea149f9c70",
   "metadata": {},
   "source": [
    "## Adding multiple heads, and finish the whole block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "067fa56c-2488-4a55-a932-07fd91229250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fb9f8c45-f383-43c1-b473-f888c7edb32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017313 M parameters\n"
     ]
    }
   ],
   "source": [
    "n_layer=1\n",
    "n_head=4\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "11b08403-e2ab-4271-b1db-09d4bd002dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1729, val loss 4.1737\n",
      "step 2000: train loss 2.2888, val loss 2.3606\n",
      "step 4000: train loss 2.2972, val loss 2.4150\n",
      "step 6000: train loss 2.2463, val loss 2.2001\n",
      "step 8000: train loss 2.3014, val loss 2.1779\n"
     ]
    }
   ],
   "source": [
    "run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2f0556cf-3b1b-4041-948e-b74bd3ce7399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hist intw'd,\n",
      "Ajk:\n",
      "CLifple crow, Loret.\n",
      "Dit\n",
      "To prey mer lout I sio he tig;\n",
      "the meinkn Mastinsor romy net sielffet as EO:\n",
      "Frse not thour y dey,\n",
      "Thenr whig andst brengeste mis sthe raing digher.t\n",
      "\n",
      "Hat IV:\n",
      "Whartin?\n",
      "\n",
      "\n",
      "IFor sing!\n",
      "\n",
      "But ne,\n",
      "Tand ank hell I nyeans Iff I Ras ge thy ninrngss; wompers amenennd st gum mys\n",
      "We IO hy bret, ast perif,\n",
      "Thing, rem re'd bect lond is'd, se this fayth's\n",
      "I thAnd drelect forsse,\n",
      "VI I:\n",
      "St prall lejue\n",
      "The reslay.\n",
      "\n",
      "RAMODs\n",
      "Yefongher:\n",
      "ond win hitat his. Chan'd Shim:\n",
      "This di\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50c10e-b462-4095-996a-4f0921853f28",
   "metadata": {},
   "source": [
    "## Just scale up!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5e45df4c-2a1d-417e-a12f-624ace3a490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length (here: number of characters) for predictions?\n",
    "learning_rate = 3e-4\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "max_iters = 3000\n",
    "eval_interval=max_iters//10\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "11229dfc-3a70-4fd5-971d-481d1282ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # Re-train?\n",
    "    # Takes about 15 min on V100.\n",
    "    run_training_loop()\n",
    "    torch.save(m.state_dict(), 'shakespeare_gpt.pth')\n",
    "# step 0: train loss 4.3823, val loss 4.3756\n",
    "# step 500: train loss 1.7285, val loss 1.8738\n",
    "# step 1000: train loss 1.4017, val loss 1.6253\n",
    "# step 1500: train loss 1.2790, val loss 1.5399\n",
    "# step 2000: train loss 1.1935, val loss 1.5008\n",
    "# step 2500: train loss 1.1329, val loss 1.4952\n",
    "# step 3000: train loss 1.0720, val loss 1.5008\n",
    "\n",
    "else:\n",
    "    model = GPTLanguageModel()  # Replace with your model's class\n",
    "    model.load_state_dict(torch.load('shakespeare_gpt.pth', map_location=torch.device(device)))\n",
    "    model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c395e-b990-4da8-a588-7d2d61db7ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22b8023d-036b-4bd6-8df0-9b2e07c72514",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Now, we'll dive into the model specifics and see a little trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fa54735e-ef13-45cd-96a7-669ff3b3ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words (convert each character to its token ID)\n",
    "words= [\"ROMEO\", \"JULIET\", \"ELIZABETH\",\"queen\", \"love\"]\n",
    "tokenized_words = [[stoi[char] for char in word] for word in words]\n",
    "\n",
    "# Convert to tensor and pad sequences for equal length\n",
    "max_len = max(len(t) for t in tokenized_words)\n",
    "padded_tokens = [t + [0] * (max_len - len(t)) for t in tokenized_words]\n",
    "input_ids = torch.tensor(padded_tokens).to(device)\n",
    "\n",
    "# Forward pass\n",
    "outputs = m(input_ids)\n",
    "\n",
    "# Aggregating character embeddings to get word embeddings\n",
    "# Here, using simple averaging\n",
    "words_emb = outputs[0].mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d5534cc5-1961-4465-a6a4-2ea3768a996e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET-ROMEO: 0.94\n",
      "ELIZABETH-ROMEO: 0.71\n",
      "ELIZABETH-JULIET: 0.81\n",
      "queen-ROMEO: 0.36\n",
      "queen-JULIET: 0.39\n",
      "queen-ELIZABETH: 0.43\n",
      "love-ROMEO: 0.63\n",
      "love-JULIET: 0.61\n",
      "love-ELIZABETH: 0.50\n",
      "love-queen: 0.91\n"
     ]
    }
   ],
   "source": [
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "for i in range(len(words)):\n",
    "    for j in range(i):\n",
    "        sim = cos(words_emb[i], words_emb[j])\n",
    "        print(f\"{words[i]}-{words[j]}: {sim:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ad043-0007-43cd-a73e-48dc557a7f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a72399-3b92-4f4a-8bd7-7d8773001b33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
