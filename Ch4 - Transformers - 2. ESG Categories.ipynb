{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXPV1oXIv6bM"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6HzW9Inv6bO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek9qJAMHv6bO"
      },
      "outputs": [],
      "source": [
        "!pip install datasets accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQa4MYR3v6bP"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import string\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import (ConfusionMatrixDisplay, classification_report,\n",
        "                             confusion_matrix)\n",
        "from tabulate import tabulate\n",
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "punctuations = string.punctuation\n",
        "\n",
        "try:\n",
        "  nlp = spacy.load(\"en_core_web_md\")\n",
        "except:\n",
        "  spacy.cli.download(\"en_core_web_md\")\n",
        "  nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "\n",
        "def spacy_tokenizer(sentence):\n",
        "    \"\"\"\n",
        "    Tokenises a sentence using spaCy.\n",
        "    Parameters:\n",
        "    - sentence: str, the sentence to tokenise\n",
        "    Returns:\n",
        "    - mytokens: list, the list of tokens\n",
        "    \"\"\"\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
        "    tokens = nlp(sentence[\"text\"].lower())\n",
        "\n",
        "    # Remove OOV words\n",
        "    tokens = [word for word in tokens if not word.is_oov]\n",
        "\n",
        "    # Lemmatise + lower case\n",
        "    tokens = [\n",
        "        word.lemma_.strip() if word.lemma_ != \"-PRON-\" else word.lower_\n",
        "        for word in tokens\n",
        "    ]\n",
        "\n",
        "    # Remove stop words\n",
        "    tokens = [\n",
        "        word for word in tokens if word not in stop_words and word not in punctuations\n",
        "    ]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def log_experiment_results(experiment_name, stats, filename=\"experiment_log.md\"):\n",
        "    \"\"\"\n",
        "    Appends experiment results and statistics to a markdown log file.\n",
        "\n",
        "    Parameters:\n",
        "    - experiment_name: str, the name of the experiment\n",
        "    - stats: dict, a dictionary containing the statistics to log\n",
        "    - filename: str, the path to the log file\n",
        "    \"\"\"\n",
        "    stats[\"timestamp\"] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
        "    stats[\"Experiment Name\"] = experiment_name\n",
        "    try:\n",
        "\n",
        "        df = pd.read_table(filename, sep=\"|\", skipinitialspace=True).drop(0)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
        "\n",
        "    except (FileNotFoundError, pd.errors.EmptyDataError, pd.errors.ParserError):\n",
        "        df = pd.DataFrame(columns=list(stats.keys()))\n",
        "\n",
        "    df = pd.concat([df, pd.DataFrame([stats])], ignore_index=True)\n",
        "    df = df[\n",
        "        [\"precision\", \"recall\", \"f1-score\", \"support\", \"timestamp\", \"Experiment Name\"]\n",
        "    ]\n",
        "    markdown_table = tabulate(\n",
        "        df,\n",
        "        headers=\"keys\",\n",
        "        tablefmt=\"pipe\",\n",
        "        showindex=False,\n",
        "        floatfmt=(\".3g\"),\n",
        "        intfmt=\",\",\n",
        "    )\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(markdown_table)\n",
        "\n",
        "\n",
        "def evaluate_model(y_test, predictions, classes):\n",
        "    \"\"\"\n",
        "    Prints classification report and confusion matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - y_test: list, the true labels\n",
        "    - predictions: list, the predicted labels\n",
        "    - classes: list, the list of classes\n",
        "\n",
        "    Returns:\n",
        "    - stats: dict, the classification report\n",
        "    \"\"\"\n",
        "    stats = classification_report(y_test, predictions, output_dict=True)\n",
        "    print(classification_report(y_test, predictions))\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "    cmp = ConfusionMatrixDisplay(\n",
        "        confusion_matrix(y_test, predictions),\n",
        "        display_labels=classes,\n",
        "    )\n",
        "\n",
        "    cmp.plot(ax=ax)\n",
        "    plt.show()\n",
        "    return stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vW9svTE289D"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, Features, ClassLabel, Value\n",
        "from transformers import DataCollatorWithPadding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90nNPXyXv6bQ"
      },
      "source": [
        "## Get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HuEhaD-v6bQ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/JosPolfliet/vlerick-mai-nlp-2023/main/DATA/esg_reports.csv\")\n",
        "df[\"labels\"] = df[\"subject\"].fillna(\"Other\")\n",
        "df[\"labels\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oMvVUMnv6bQ"
      },
      "source": [
        "## Transformers model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLuHZuuBv6bQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hspevMXLv6bR"
      },
      "outputs": [],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUZs76o5v6bR"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
        "\n",
        "sentence = df.iloc[203][\"text\"]\n",
        "encoded = preprocess_function({\"text\":[sentence]})\n",
        "print(sentence)\n",
        "print(encoded)\n",
        "for w in encoded['input_ids'][0]:\n",
        "    print(w, tokenizer.decode([w]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55MeYRzHv6bS"
      },
      "outputs": [],
      "source": [
        "for w in encoded['input_ids'][0]:\n",
        "    print(w, tokenizer.decode([w]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EomeAN58v6bS"
      },
      "outputs": [],
      "source": [
        "class_names = [\"Environmental\", \"Social\", \"None\"]\n",
        "esg_classes = Features({'__index_level_0__': Value('string'),\n",
        "                             'text': Value('string'),\n",
        "                             'labels': ClassLabel(names=class_names)})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCWB7uXuv6bS"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX7DBcUNv6bS"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split(df, random_state=22141, stratify=df[\"labels\"])\n",
        "train = Dataset.from_pandas(df_train[[\"text\", \"labels\"]], features=esg_classes)\n",
        "test = Dataset.from_pandas(df_test[[\"text\", \"labels\"]], features=esg_classes)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVEVOkv9v6bT"
      },
      "outputs": [],
      "source": [
        "tokenized_train = train.map(preprocess_function, batched=True)\n",
        "tokenized_test = test.map(preprocess_function, batched=True)\n",
        "tokenized_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hn9D9xUv6bT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msqKmzqvv6bT"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=20,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch\n",
        "    eval_steps=5,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5OVWfQUv6bT"
      },
      "source": [
        "This takes a long time. I trained this in the cloud instead, results are in the lecture powerpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hB4OK3_v6bT"
      },
      "outputs": [],
      "source": [
        "# If you are runnign for real, save your work!\n",
        "# model.save_pretrained(\"mymodel\")\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"mymodel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K3Enfbev6bT"
      },
      "source": [
        "## Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mThaJVaVv6bT"
      },
      "outputs": [],
      "source": [
        "\n",
        "experiment_name = \"Transformers dummy\"\n",
        "\n",
        "predictions = trainer.predict(tokenized_test)\n",
        "prediction_labels = [class_names[i] for i in predictions.predictions.argmax(-1)]\n",
        "\n",
        "stats = evaluate_model(df_test[\"labels\"], prediction_labels, class_names)\n",
        "log_experiment_results(experiment_name, stats[\"macro avg\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3LGrYdNv6bT"
      },
      "outputs": [],
      "source": [
        "predictions = trainer.predict(tokenized_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OA9sOFav6bT"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q_Bc9IKv6bT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ5Ruyo6v6bT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "nlpdemystified-preprocessing.ipynb",
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}