{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXPV1oXIv6bM"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6HzW9Inv6bO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek9qJAMHv6bO"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQa4MYR3v6bP"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import requests\n",
        "  response = requests.get(\"https://raw.githubusercontent.com/JosPolfliet/vlerick-mai-nlp-2023/main/mai_nlp_helper_functions.py\")\n",
        "  print(response.status_code)\n",
        "  with open(\"mai_nlp_helper_functions.py\", 'wb') as file:\n",
        "      file.write(response.content)\n",
        "\n",
        "  from mai_nlp_helper_functions import *\n",
        "except ImportError as e:\n",
        "  raise ImportError(\"You don't have the mai_nlp_helper_functions.py file in the same directory as your note book. Either add it, or copy paste the contents in this cell\") from e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vW9svTE289D"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, Features, ClassLabel, Value\n",
        "from transformers import DataCollatorWithPadding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90nNPXyXv6bQ"
      },
      "source": [
        "## Get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HuEhaD-v6bQ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/JosPolfliet/vlerick-mai-nlp-2023/main/DATA/esg_reports.csv\")\n",
        "df[\"labels\"] = df[\"subject\"].fillna(\"Other\")\n",
        "df[\"labels\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oMvVUMnv6bQ"
      },
      "source": [
        "## Transformers model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLuHZuuBv6bQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hspevMXLv6bR"
      },
      "outputs": [],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUZs76o5v6bR"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
        "\n",
        "sentence = df.iloc[203][\"text\"]\n",
        "encoded = preprocess_function({\"text\":[sentence]})\n",
        "print(sentence)\n",
        "print(encoded)\n",
        "for w in encoded['input_ids'][0]:\n",
        "    print(w, tokenizer.decode([w]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55MeYRzHv6bS"
      },
      "outputs": [],
      "source": [
        "for w in encoded['input_ids'][0]:\n",
        "    print(w, tokenizer.decode([w]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EomeAN58v6bS"
      },
      "outputs": [],
      "source": [
        "class_names = [\"Environmental\", \"Social\", \"None\"]\n",
        "esg_classes = Features({'__index_level_0__': Value('string'),\n",
        "                             'text': Value('string'),\n",
        "                             'labels': ClassLabel(names=class_names)})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCWB7uXuv6bS"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX7DBcUNv6bS"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split(df, random_state=22141, stratify=df[\"labels\"])\n",
        "train = Dataset.from_pandas(df_train[[\"text\", \"labels\"]], features=esg_classes)\n",
        "test = Dataset.from_pandas(df_test[[\"text\", \"labels\"]], features=esg_classes)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVEVOkv9v6bT"
      },
      "outputs": [],
      "source": [
        "tokenized_train = train.map(preprocess_function, batched=True)\n",
        "tokenized_test = test.map(preprocess_function, batched=True)\n",
        "tokenized_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hn9D9xUv6bT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msqKmzqvv6bT"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=20,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5OVWfQUv6bT"
      },
      "source": [
        "This takes a long time. I trained this in the cloud instead, results are in the lecture powerpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hB4OK3_v6bT"
      },
      "outputs": [],
      "source": [
        "# If you are runnign for real, save your work!\n",
        "# model.save_pretrained(\"mymodel\")\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"mymodel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K3Enfbev6bT"
      },
      "source": [
        "## Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mThaJVaVv6bT"
      },
      "outputs": [],
      "source": [
        "\n",
        "experiment_name = \"Transformers dummy\"\n",
        "\n",
        "predictions = trainer.predict(tokenized_test)\n",
        "prediction_labels = [class_names[i] for i in predictions.predictions.argmax(-1)]\n",
        "\n",
        "stats = evaluate_model(df_test[\"labels\"], prediction_labels, class_names)\n",
        "log_experiment_results(experiment_name, stats[\"macro avg\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3LGrYdNv6bT"
      },
      "outputs": [],
      "source": [
        "predictions = trainer.predict(tokenized_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OA9sOFav6bT"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q_Bc9IKv6bT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ5Ruyo6v6bT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "nlpdemystified-preprocessing.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}