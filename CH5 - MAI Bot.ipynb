{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cd0adc-60ff-4e8c-9a36-43f59ce578de",
   "metadata": {},
   "source": [
    "# Sentence embeddings\n",
    "We will mainly use `sentence-transformers`, which is a dedicated package from Hugging Face ğŸ¤—. \n",
    "\n",
    "Relevant documentation\n",
    "- Semantic textual similarity https://www.sbert.net/docs/usage/semantic_textual_similarity.html\n",
    "- Semantic search https://www.sbert.net/examples/applications/semantic-search/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f070d4c-1c9b-4af9-9b22-d357e07c0ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mlangchain==0.0.344\n",
      "langchain-core==0.0.8\n",
      "langdetect==1.0.9\n",
      "langsmith==0.0.67\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e36749f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentence-transformers in /opt/homebrew/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: faiss-cpu in /opt/homebrew/lib/python3.11/site-packages (1.7.4)\n",
      "Requirement already satisfied: langchain in /opt/homebrew/lib/python3.11/site-packages (0.0.342)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.0.344-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: openai in /opt/homebrew/lib/python3.11/site-packages (1.3.6)\n",
      "Collecting openai\n",
      "  Downloading openai-1.3.7-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: nest-asyncio in /Users/jospolfliet/Library/Python/3.11/lib/python/site-packages (1.5.8)\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.29.0-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: unstructured[md] in /opt/homebrew/lib/python3.11/site-packages (0.11.0)\n",
      "Collecting unstructured[md]\n",
      "  Downloading unstructured-0.11.2-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (4.35.2)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (0.16.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (0.19.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: anyio<4.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Collecting langchain-core<0.1,>=0.0.8 (from langchain)\n",
      "  Downloading langchain_core-0.0.8-py3-none-any.whl.metadata (750 bytes)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (0.0.67)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: chardet in /opt/homebrew/lib/python3.11/site-packages (from unstructured[md]) (5.2.0)\n",
      "Requirement already satisfied: filetype in /opt/homebrew/lib/python3.11/site-packages (from unstructured[md]) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /opt/homebrew/lib/python3.11/site-packages (from unstructured[md]) (0.4.27)\n",
      "Requirement already satisfied: lxml in /opt/homebrew/lib/python3.11/site-packages (from unstructured[md]) (4.9.3)\n",
      "Requirement already satisfied: tabulate in /opt/homebrew/lib/python3.11/site-packages (from unstructured[md]) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/lib/python3.11/site-packages (from unstructured[md]) (4.12.2)\n",
      "Requirement already satisfied: emoji in /opt/homebrew/lib/python3.11/site-packages (from unstructured[md]) (2.8.0)\n",
      "Requirement already satisfied: python-iso639 in /opt/homebrew/lib/python3.11/site-packages (from unstructured[md]) (2023.6.15)\n",
      "Requirement already satisfied: langdetect in /opt/homebrew/lib/python3.11/site-packages (from unstructured[md]) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in /opt/homebrew/lib/python3.11/site-packages (from unstructured[md]) (3.5.2)\n",
      "Requirement already satisfied: backoff in /opt/homebrew/lib/python3.11/site-packages (from unstructured[md]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from unstructured[md]) (4.7.1)\n",
      "Requirement already satisfied: wrapt in /opt/homebrew/lib/python3.11/site-packages (from unstructured[md]) (1.16.0)\n",
      "Requirement already satisfied: markdown in /opt/homebrew/lib/python3.11/site-packages (from unstructured[md]) (3.5.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from openai) (0.24.1)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from streamlit) (1.6.3)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/homebrew/lib/python3.11/site-packages (from streamlit) (8.1.7)\n",
      "Collecting importlib-metadata<7,>=1.4 (from streamlit)\n",
      "  Downloading importlib_metadata-6.9.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in /Users/jospolfliet/Library/Python/3.11/lib/python/site-packages (from streamlit) (23.2)\n",
      "Collecting pandas<3,>=1.3.0 (from streamlit)\n",
      "  Downloading pandas-2.1.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /opt/homebrew/lib/python3.11/site-packages (from streamlit) (10.1.0)\n",
      "Collecting protobuf<5,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Collecting pyarrow>=6.0 (from streamlit)\n",
      "  Downloading pyarrow-14.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /opt/homebrew/lib/python3.11/site-packages (from streamlit) (2.8.2)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tzlocal<6,>=1.1 (from streamlit)\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit)\n",
      "  Downloading validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /Users/jospolfliet/Library/Python/3.11/lib/python/site-packages (from streamlit) (6.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/homebrew/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (4.20.0)\n",
      "Collecting toolz (from altair<6,>=4.0->streamlit)\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.11/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /opt/homebrew/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (0.17.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<7,>=1.4->streamlit)\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Collecting pytz>=2020.1 (from pandas<3,>=1.3.0->streamlit)\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas<3,>=1.3.0->streamlit)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.14.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/jospolfliet/Library/Python/3.11/lib/python/site-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.11/site-packages (from beautifulsoup4->unstructured[md]) (2.5)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.11/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/lib/python3.11/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.11.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.31.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.13.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Downloading langchain-0.0.344-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.3.7-py3-none-any.whl (221 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-6.9.0-py3-none-any.whl (22 kB)\n",
      "Downloading langchain_core-0.0.8-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.1.3-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-14.0.1-cp311-cp311-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Downloading unstructured-0.11.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pytz, zipp, validators, tzlocal, tzdata, toolz, toml, smmap, pyarrow, protobuf, mdurl, cachetools, pydeck, pandas, markdown-it-py, importlib-metadata, gitdb, unstructured, rich, gitpython, openai, langchain-core, altair, streamlit, langchain\n",
      "  Attempting uninstall: unstructured\n",
      "\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: unstructured 0.11.0\n",
      "    Uninstalling unstructured-0.11.0:\n",
      "      Successfully uninstalled unstructured-0.11.0\n",
      "  Attempting uninstall: openai\n",
      "\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: openai 1.3.6\n",
      "    Uninstalling openai-1.3.6:\n",
      "      Successfully uninstalled openai-1.3.6\n",
      "  Attempting uninstall: langchain-core\n",
      "\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: langchain-core 0.0.7\n",
      "    Uninstalling langchain-core-0.0.7:\n",
      "      Successfully uninstalled langchain-core-0.0.7\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.342\n",
      "    Uninstalling langchain-0.0.342:\n",
      "      Successfully uninstalled langchain-0.0.342\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/packaging-23.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed altair-5.2.0 cachetools-5.3.2 gitdb-4.0.11 gitpython-3.1.40 importlib-metadata-6.9.0 langchain-0.0.344 langchain-core-0.0.8 markdown-it-py-3.0.0 mdurl-0.1.2 openai-1.3.7 pandas-2.1.3 protobuf-4.25.1 pyarrow-14.0.1 pydeck-0.8.1b0 pytz-2023.3.post1 rich-13.7.0 smmap-5.0.1 streamlit-1.29.0 toml-0.10.2 toolz-0.12.0 tzdata-2023.3 tzlocal-5.2 unstructured-0.11.2 validators-0.22.0 zipp-3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers faiss-cpu langchain  \"unstructured[md]\" openai nest-asyncio streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10fd65a-d607-4a2f-8e46-c07bb71ec11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ee6e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.76214680e-01,  1.20601490e-01, -2.93624043e-01,\n",
       "        -2.29858160e-01, -8.22923556e-02,  2.37709701e-01,\n",
       "         3.39984596e-01, -7.80964196e-01,  1.18127435e-01,\n",
       "         1.63373843e-01, -1.37715429e-01,  2.40282565e-01,\n",
       "         4.25125778e-01,  1.72417641e-01,  1.05280034e-01,\n",
       "         5.18164277e-01,  6.22214526e-02,  3.99285913e-01,\n",
       "        -1.81652635e-01, -5.85578501e-01,  4.49724011e-02,\n",
       "        -1.72750384e-01, -2.68443584e-01, -1.47386163e-01,\n",
       "        -1.89217702e-01,  1.92150414e-01, -3.83842826e-01,\n",
       "        -3.96007091e-01,  4.30648834e-01, -3.15320015e-01,\n",
       "         3.65949929e-01,  6.05159178e-02,  3.57325375e-01,\n",
       "         1.59736529e-01, -3.00983638e-01,  2.63250142e-01,\n",
       "        -3.94310504e-01,  1.84855461e-01, -3.99549633e-01,\n",
       "        -2.67889559e-01, -5.45117497e-01, -3.13404575e-02,\n",
       "        -4.30644214e-01,  1.33278072e-01, -1.74793854e-01,\n",
       "        -4.35465217e-01, -4.77379173e-01,  7.12554380e-02,\n",
       "        -7.37003982e-02,  5.69136977e-01, -2.82579482e-01,\n",
       "         5.24978302e-02, -8.20007920e-01,  1.98296472e-01,\n",
       "         1.69512168e-01,  2.71780789e-01,  2.64610589e-01,\n",
       "        -2.55739000e-02, -1.74096167e-01,  1.63314581e-01,\n",
       "        -3.95261168e-01, -3.17555256e-02, -2.62555957e-01,\n",
       "         3.52754384e-01,  3.01434934e-01, -1.47196919e-01,\n",
       "         2.10075468e-01, -1.84010431e-01, -4.12896216e-01,\n",
       "         4.14775729e-01, -1.89769819e-01, -1.35482669e-01,\n",
       "        -3.79272401e-01, -4.68021818e-02, -3.33601609e-02,\n",
       "         9.00392011e-02, -3.30133438e-01, -3.87314633e-02,\n",
       "         3.75082552e-01, -1.46996304e-01,  4.34959739e-01,\n",
       "         5.38325727e-01, -2.65445292e-01,  1.64446041e-01,\n",
       "         4.17078048e-01, -4.72509861e-02, -7.48728365e-02,\n",
       "        -4.26261127e-01, -1.96994230e-01,  6.10317551e-02,\n",
       "        -4.74262446e-01, -6.48334742e-01,  3.71462524e-01,\n",
       "         2.50957191e-01,  1.22529417e-01,  8.88767317e-02,\n",
       "        -1.06724396e-01,  5.33984080e-02,  9.74505693e-02,\n",
       "        -3.46661843e-02, -1.02882691e-01,  2.32289061e-01,\n",
       "        -2.53739715e-01, -5.13111651e-01,  1.85216591e-01,\n",
       "        -3.04357708e-01, -3.55205201e-02, -1.26975372e-01,\n",
       "        -7.71630853e-02, -5.15329897e-01, -2.28071555e-01,\n",
       "         2.03345418e-02,  7.38176405e-02, -1.52558506e-01,\n",
       "        -4.00837749e-01, -2.47748941e-01,  3.97470742e-01,\n",
       "        -2.60260910e-01,  2.50905842e-01,  1.68228939e-01,\n",
       "         1.33900583e-01, -2.10834350e-02, -4.70035374e-01,\n",
       "         4.78850216e-01,  2.80345708e-01, -4.64546949e-01,\n",
       "         3.21746886e-01,  2.34207451e-01,  2.45772347e-01,\n",
       "        -4.71482277e-01,  5.00401378e-01,  4.10189956e-01,\n",
       "         5.15217245e-01,  2.62549430e-01,  2.11593248e-02,\n",
       "        -3.89687389e-01, -2.41742998e-01, -2.14834392e-01,\n",
       "        -8.62654001e-02, -1.65323764e-01, -5.21897078e-02,\n",
       "         3.41875523e-01,  4.50314373e-01, -3.06973666e-01,\n",
       "        -2.02293858e-01,  6.85521960e-01, -5.33892214e-01,\n",
       "         3.58471900e-01,  1.45287007e-01, -7.07057938e-02,\n",
       "        -1.50529146e-01, -8.56281146e-02, -7.67854825e-02,\n",
       "         1.89544663e-01, -1.04067445e-01,  5.33543885e-01,\n",
       "        -5.27886868e-01,  2.42333245e-02, -2.64348060e-01,\n",
       "        -2.23186910e-01, -3.81208777e-01,  7.59911463e-02,\n",
       "        -4.64484870e-01, -3.36549014e-01,  4.21229690e-01,\n",
       "         1.07479304e-01,  1.90457731e-01,  2.89492263e-03,\n",
       "        -1.08513966e-01,  1.53545633e-01,  3.16023529e-01,\n",
       "        -2.70840339e-02, -5.40594935e-01,  8.97287950e-02,\n",
       "        -1.15549281e-01,  3.97803664e-01, -4.97683465e-01,\n",
       "        -2.84893543e-01,  4.99864556e-02,  3.61279339e-01,\n",
       "         6.90535426e-01,  1.46821737e-01,  1.73396453e-01,\n",
       "        -1.74582571e-01, -3.15702558e-01,  6.72997758e-02,\n",
       "         2.17250213e-01,  9.78537053e-02, -1.29472643e-01,\n",
       "        -1.86929554e-01,  1.34878382e-01, -1.53885469e-01,\n",
       "         7.44719878e-02, -1.85536325e-01, -2.80627936e-01,\n",
       "        -1.14144176e-01,  4.12249386e-01,  6.39491379e-02,\n",
       "        -1.45715609e-01, -9.82060730e-02, -1.33082107e-01,\n",
       "        -1.88410416e-01, -2.84841154e-02, -3.49510051e-02,\n",
       "         3.34261879e-02,  6.98897094e-02,  1.90354377e-01,\n",
       "        -2.96723872e-01,  2.64710188e-03,  1.09140903e-01,\n",
       "         1.70894098e-02,  2.60588974e-01,  3.29038173e-01,\n",
       "        -6.61558211e-02,  2.39665270e-01, -2.26194531e-01,\n",
       "        -3.36871259e-02,  1.49400055e-01, -3.21265161e-01,\n",
       "        -2.68578082e-01,  5.72631836e-01, -4.92308766e-01,\n",
       "         2.00666413e-01, -3.49261552e-01, -2.89885122e-02,\n",
       "         6.09010220e-01, -5.72333395e-01,  2.35000432e-01,\n",
       "         6.47259783e-03, -3.14949602e-02,  2.78107226e-02,\n",
       "        -3.90340656e-01, -2.08950073e-01, -3.04452956e-01,\n",
       "        -7.20200092e-02, -8.29840899e-02,  3.73792946e-01,\n",
       "         7.38933608e-02, -2.21072529e-02,  9.88138765e-02,\n",
       "        -1.51427165e-01, -1.40430421e-01,  2.26018056e-01,\n",
       "         2.76089907e-01, -8.87746662e-02, -1.12816371e-01,\n",
       "        -2.66285866e-01,  2.77834743e-01, -4.75605913e-02,\n",
       "         6.71002418e-02, -2.78583486e-02, -2.39992645e-02,\n",
       "         2.51708895e-01,  4.68793809e-01, -5.39325416e-01,\n",
       "         1.10598579e-01, -3.44947219e-01,  4.15990233e-01,\n",
       "         7.28482306e-02, -3.19647461e-01,  4.90374535e-01,\n",
       "        -7.30271870e-03, -2.64247344e-03,  9.63710845e-01,\n",
       "         3.23885083e-01, -7.79614821e-02, -2.37588942e-01,\n",
       "         2.34038725e-01, -3.16053867e-01, -1.65647641e-03,\n",
       "        -1.09070671e+00,  3.38409156e-01,  4.70605567e-02,\n",
       "         1.07435599e-01, -2.06672043e-01,  4.26444458e-03,\n",
       "        -1.38453743e-03, -5.31455636e-01, -2.75648624e-01,\n",
       "        -1.64648801e-01, -3.42916906e-01, -4.26118493e-01,\n",
       "         6.01812005e-01,  4.55972075e-01, -2.72702128e-01,\n",
       "        -3.45804431e-02,  2.62752175e-01, -6.34184759e-03,\n",
       "         2.79631078e-01, -2.53558964e-01, -1.68626532e-01,\n",
       "         3.82932648e-02,  2.07762942e-01, -4.31526035e-01,\n",
       "        -7.24001005e-02, -1.26854792e-01,  2.07031444e-02,\n",
       "         5.74441552e-01,  3.54672700e-01,  9.28294584e-02,\n",
       "         6.70505539e-02,  1.11520849e-01, -1.86516065e-02,\n",
       "         4.62352157e-01,  2.72504807e-01, -3.60474169e-01,\n",
       "         5.29415429e-01, -1.00316911e-03, -8.81361663e-02,\n",
       "         1.49975225e-01,  5.25866002e-02,  4.63517487e-01,\n",
       "        -3.96831691e-01,  2.42640898e-01, -2.08912924e-01,\n",
       "         3.65672112e-01, -4.73535969e-04,  5.33963323e-01,\n",
       "        -1.97879568e-01,  3.11582774e-01, -6.96715117e-01,\n",
       "        -4.29500461e-01, -4.49359447e-01, -2.71373522e-02,\n",
       "        -6.98713064e-02,  2.06175357e-01, -1.57107368e-01,\n",
       "         4.43521082e-01, -6.74266070e-02, -3.00924122e-01,\n",
       "         5.14859259e-01,  3.36029679e-01,  6.63373917e-02,\n",
       "        -1.15235299e-01, -2.95981281e-02,  2.79471844e-01,\n",
       "        -3.48198079e-02, -7.29327351e-02, -4.58471775e-02,\n",
       "         1.54262796e-01,  8.09356153e-01,  5.20328224e-01,\n",
       "        -4.02114809e-01, -3.23150493e-02, -1.10364184e-01,\n",
       "         7.50503466e-02, -1.51098385e-01,  8.45740080e-01,\n",
       "        -1.80843860e-01,  3.22573662e-01,  1.04708359e-01,\n",
       "         3.19663346e-01, -1.55085415e-01,  1.69236928e-01,\n",
       "        -2.56996810e-01,  2.01208591e-01,  1.77392885e-01,\n",
       "        -2.74333209e-01, -3.36944759e-01,  5.02357185e-01,\n",
       "        -1.18357547e-01, -2.01166764e-01, -5.36486030e-01,\n",
       "        -7.69812167e-02,  1.15388837e-02, -2.36464605e-01,\n",
       "        -2.98771895e-02,  1.31366342e-01,  2.94184595e-01,\n",
       "         9.90916789e-02, -5.43897688e-01,  1.40812665e-01,\n",
       "         3.66998643e-01,  5.04861847e-02,  1.99122176e-01,\n",
       "        -2.80674636e-01,  4.34192032e-01, -1.40274823e-01,\n",
       "         5.78049183e-01,  1.77715674e-01,  8.98362100e-02,\n",
       "         3.29651207e-01,  6.13005981e-02, -3.24933499e-01]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Sentences we want to encode. Example:\n",
    "sentence = ['This framework generates embeddings for each input sentence']\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "embedding = model.encode(sentence)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0ba9e8-82a7-458b-97ab-d9af7f2555a5",
   "metadata": {},
   "source": [
    "See, a sentence embedding is just a vector, just like a word embedding. That means we can also calculate similarities in a similar way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5113acd-53a0-47bd-a7b6-d1f072b0baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome!']\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',\n",
    "              'My plants look a bit sick, could it be bitrot?',\n",
    "              'The new movie is so great!']\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f74a88-c711-4b1b-a7cc-8a7a473c2478",
   "metadata": {},
   "source": [
    "## Semantic search and retrieval\n",
    "\n",
    "The idea behind semantic search is to embed all entries in your corpus, whether they be sentences, paragraphs, or documents, into a vector space.\n",
    "\n",
    "At search time, the query is embedded into the same vector space and the closest embeddings from your corpus are found. These entries should have a high semantic overlap with the query.\n",
    "\n",
    "\n",
    "![title](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/SemanticSearch.png\n",
    ")\n",
    "\n",
    "Instead of trying to build a semantic search engine from first principles, we'll use `langchain`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1fe379d1-fc8d-400c-af48-46b22cde31c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching pages:   0%|                                                                                                                         | 0/108 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching pages:  15%|################5                                                                                               | 16/108 [00:01<00:06, 14.91it/s]\u001b[A\n",
      "Fetching pages:  31%|##################################2                                                                             | 33/108 [00:02<00:04, 15.54it/s]\u001b[A\n",
      "Fetching pages:  45%|##################################################8                                                             | 49/108 [00:03<00:03, 15.08it/s]\u001b[A\n",
      "Fetching pages:  61%|####################################################################4                                           | 66/108 [00:04<00:02, 15.47it/s]\u001b[A\n",
      "Fetching pages:  76%|#####################################################################################                           | 82/108 [00:05<00:01, 15.65it/s]\u001b[A\n",
      "Fetching pages: 100%|###############################################################################################################| 108/108 [00:07<00:00, 15.38it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='â”\\nWhat is Metamaze?\\nOn a mission to liberate mankind from repetitive document and e-mail processing.\\nMetamaze is a platform for building semi-automated flows for processing any type of document or e-mail. Metamaze enables companies to automate large parts of repetitive data entry and validation tasks. \\nBy using Metamaze, companies can \\nautomate 50 to 98% of the manual work. \\nThis leads to\\nimproved employee well-being \\nlower labor costs\\nmore time for value-adding activities\\nthe unlocking of new data and insights\\nAdapts to your process, and your documents \\nAdaptive IDP platforms like Metamaze are flexible systems that adapt to \\nyour process\\n and learn through \\nfully integrated human feedback\\n. Metamaze is not a rigid off-the-shelf â€˜one-size fits no-oneâ€™ solution. \\nYou can automate any document type you want, including fully custom document types.\\nYou can start from scratch with as little as 10 examples, or start fine-tuning an existing model from our vast library of document types. \\nYou can add your own custom fields to new or existing document types. Metamaze supports all types including simple text, line items, complex hierarchical tables, paragraphs, RegEx, and images.\\nYou can include external data lookup or validation by using data enrichments.\\nYou can customize the pipeline with any custom code. \\nMultilingual by design, with 50+ languages supported.\\nBest-in-class AI technology\\nThe Metamaze Hydra model heavily uses\\n\\u202fpre-trained transformer models. \\nThis technology allows the training and processing of both simple, complex, and fully unstructured documents. You can read more about the latest model architecture \\nhere\\n.\\nWhether you wish to process invoices, purchase orders, complex legal agreements, or any custom document type, Metamaze intelligently interprets documents with a near-human level of precision.\\nBuilt for humans\\nMetamaze is designed to be used by any business user and does not require coding or AI skills to configure, release or maintain.\\nMetamaze is an intuitive platform with a \\nvalidation interface usable by anyone\\n, including non-IT-savvy data entry people.\\nSmart, configurable system to decide when human intervention is required\\nComes with wheels attached: model training, evaluation, version management, deployment, configuration, input/output, user management, â€¦ are all built-in. And all of that without the need for any coding or AI skills.\\nBuilt for the enterprise\\nMetamaze is built ground up to support the needs of large and complex organizations. That means\\nEnterprise-level security and compliance. See our \\nTrust Center\\n.\\nAbility to segregate your data, while still having the benefit of sharing models \\nYou can enrich and validate using external data sources like SAP, Dynamics, Salesforce, ...\\nCustomize your pipeline by writing any custom code \\nConnect to REST APIs, Outlook 365, Sharepoint, SFTP, UIPath, BluePrism, or by creating your own using the REST API \\nSupports all common file types and document formats  \\n\\u200b\\nNext\\nHome\\nLast modified \\n4mo ago', metadata={'source': 'https://docs.app.metamaze.eu/', 'title': 'What is Metamaze?'})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.document_loaders import GitbookLoader\n",
    "gitbookloader = GitbookLoader(\"https://docs.app.metamaze.eu\", load_all_paths=True).load()\n",
    "gitbookloader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2e8f7475-7847-4659-a9ba-24307d9ebbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 s, sys: 1.72 s, total: 13.3 s\n",
      "Wall time: 2min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='', metadata={'title': 'Documentation', 'id': '925499519', 'source': 'https://metamaze.atlassian.net/wiki/spaces/DO/pages/925499519/Documentation'})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.document_loaders import ConfluenceLoader\n",
    "loader = ConfluenceLoader(\n",
    "    url=\"https://metamaze.atlassian.net/wiki\", username=\"j.polfliet@metamaze.eu\", api_key=\"ATATT3xFfGF0OQk4dvOwp3divV4TO5bkwuzHv-jUVGfXus9hSag3BFfHBuIefjS8H64Qi0dgu5DoOEdDPRZaaOk_K7qSgUSra25gxbhv5WECT5Dw026_JokSMe7ovUrQgn8y4HzsvfWB-RNdGQZEvuzXh5L0nwRqEfP0H79T1hOvg85fNkfoHC0=55F53EEF\"\n",
    ")\n",
    "confluence = loader.load(space_key=\"DO\", include_attachments=False)\n",
    "confluence[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987a116-7c32-4e62-b8b0-93fdba04b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = DirectoryLoader('rfpgpt/resources/', glob=\"**/*.md\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3117d9c0-64a1-42ef-80e5-0beec692644a",
   "metadata": {},
   "source": [
    "## Create new vector store and embed all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b604e3e2-ff3f-4e40-ba93-1d55d5698bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4bfc75d6-a218-4c49-8949-35c8fed54a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 3347, which is longer than the specified 1000\n",
      "Created a chunk of size 1119, which is longer than the specified 1000\n",
      "Created a chunk of size 1070, which is longer than the specified 1000\n",
      "Created a chunk of size 1066, which is longer than the specified 1000\n",
      "Created a chunk of size 1584, which is longer than the specified 1000\n",
      "Created a chunk of size 17816, which is longer than the specified 1000\n",
      "Created a chunk of size 1670, which is longer than the specified 1000\n",
      "Created a chunk of size 3916, which is longer than the specified 1000\n",
      "Created a chunk of size 1378, which is longer than the specified 1000\n",
      "Created a chunk of size 2250, which is longer than the specified 1000\n",
      "Created a chunk of size 6484, which is longer than the specified 1000\n",
      "Created a chunk of size 1768, which is longer than the specified 1000\n",
      "Created a chunk of size 1069, which is longer than the specified 1000\n",
      "Created a chunk of size 2267, which is longer than the specified 1000\n",
      "Created a chunk of size 4310, which is longer than the specified 1000\n",
      "Created a chunk of size 1299, which is longer than the specified 1000\n",
      "Created a chunk of size 2022, which is longer than the specified 1000\n",
      "Created a chunk of size 1241, which is longer than the specified 1000\n",
      "Created a chunk of size 2037, which is longer than the specified 1000\n",
      "Created a chunk of size 2030, which is longer than the specified 1000\n",
      "Created a chunk of size 3630, which is longer than the specified 1000\n",
      "Created a chunk of size 1009, which is longer than the specified 1000\n",
      "Created a chunk of size 1304, which is longer than the specified 1000\n",
      "Created a chunk of size 1060, which is longer than the specified 1000\n",
      "Created a chunk of size 6588, which is longer than the specified 1000\n",
      "Created a chunk of size 1230, which is longer than the specified 1000\n",
      "Created a chunk of size 6346, which is longer than the specified 1000\n",
      "Created a chunk of size 2647, which is longer than the specified 1000\n",
      "Created a chunk of size 1093, which is longer than the specified 1000\n",
      "Created a chunk of size 1231, which is longer than the specified 1000\n",
      "Created a chunk of size 1609, which is longer than the specified 1000\n",
      "Created a chunk of size 1130, which is longer than the specified 1000\n",
      "Created a chunk of size 1609, which is longer than the specified 1000\n",
      "Created a chunk of size 1130, which is longer than the specified 1000\n",
      "Created a chunk of size 1375, which is longer than the specified 1000\n",
      "Created a chunk of size 2180, which is longer than the specified 1000\n",
      "Created a chunk of size 1070, which is longer than the specified 1000\n",
      "Created a chunk of size 1437, which is longer than the specified 1000\n",
      "Created a chunk of size 17763, which is longer than the specified 1000\n",
      "Created a chunk of size 1351, which is longer than the specified 1000\n",
      "Created a chunk of size 1688, which is longer than the specified 1000\n",
      "Created a chunk of size 1901, which is longer than the specified 1000\n",
      "Created a chunk of size 3669, which is longer than the specified 1000\n",
      "Created a chunk of size 1751, which is longer than the specified 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='QUESTION: If API is supported as Input Source, please indicate any possible pre-requirement (if any), \\nANSWER: For a complete description of the Metamaze REST API, please see https://app.metamaze.eu/docs/index.html', metadata={'source': 'rfpgpt/resources/faq/question_170.md'})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://python.langchain.com/docs/expression_language/cookbook/retrieval\n",
    "\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents + gitbookloader + confluence)\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ed4206-743d-4e72-b471-e2e300642645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 183 ms, sys: 102 ms, total: 284 ms\n",
      "Wall time: 316 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# to test, use query_result = embeddings.embed_query(\"My text\")\n",
    "\n",
    "if False: # change to True if you want to (re)create your store   \n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents, embedding=embeddings\n",
    "    )\n",
    "    # store because this is slow\n",
    "    vectorstore.save_local(\"vectorstore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "228e626e-4185-49b0-971e-e52cd3f792b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jospolfliet/src/vlerick'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0c74ddc-4144-4ff6-a4bf-4f64b9ef1010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlat; proxy of <Swig Object of type 'faiss::IndexFlat *' at 0x2b3329ec0> >"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = FAISS.load_local(\"vectorstore\", embeddings)\n",
    "vectorstore.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a80908f-1696-461d-98ca-f973038139dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough, RunnableMap, RunnableSequence\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "\n",
    "retriever = vectorstore.as_retriever(k=8)\n",
    "\n",
    "template = \"\"\"Answer the question using only information from the following, related previous answers or context:\n",
    "# CONTEXT:\n",
    "{context}\n",
    "# INSTRUCTIONS:\n",
    "- Replace any mentions of \"provider\", \"supplier\" or similar with \"Metamaze\"\n",
    "- Replace any mentions of \"AXA\", \"AG Insurance\", \"KBC\", or other potential client names with \"Client\"\n",
    "# QUESTION: \n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07a287cd-16f1-4d44-bb64-3506394847d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model_name=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34b68cea-d75d-4a71-891c-7593ac37695e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (3294547597.py, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[31], line 21\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"({doc.source})\"))\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: input[\"documents\"],\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain_with_source = RunnableMap(\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: input[\"documents\"],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n",
    "\n",
    "def q(s):\n",
    "    result = rag_chain_with_source.invoke(s)\n",
    "    print(f\"## Reference data\")\n",
    "    for doc in result['documents']:\n",
    "        print(f\"({doc.source})\"))\n",
    "        print(doc.page_content)\n",
    "        print(\"-------\")\n",
    "    print(f\"\\n ## Answer:\\n\\n{result['answer']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4717162b-dff5-439d-a678-0676684a5b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Reference data\n",
      "Penetration Testing Policy Penetration Testing is a legal, authorized simulated attack performed in order to evaluate the security controls of the Metamaze application and infrastructure and of the Metamaze organization while identifying the exploitable vulnerabilities as well as its strengths, enabling a full risk assessment to be completed. This policy specifies the penetration testing process steps in order to maximise the business value and minimise the security risks. Scope This policy applies throughout the organization and it is particularly relevant to the Engineering department. The objective of the penetration testing is to detect the security weaknesses that could be used by externals to gain unauthorized access to information. The testing will be performed At least on a annual basis When major releases are planned that significantly impact authorization, authentication or data tenancy. In this case, the penetration test should be performed before the release is done do production. The checklist below represents the process steps to be performed by the organization prior to the penetration test. Define scope Define threat model Define testing type: black box, white box, grey box (In a black box penetration test, no information is provided to the tester at all/White box penetration testing involves sharing full network and system information with the tester/In a grey box penetration test only limited information is shared with the tester) Define used methods (external, internal, blind, double-blind, targeted) Contract with 3rd party service for the defined scope and support chosen supplier during execution. Evaluate PEN Test report If vulnerabilities are identified: Identify root cause Define action plan Priority of correction All critical and high risks vulnerabilities must be remediated by 30 days The remediation of medium and low risk vulnerabilities can be planned as part of the overall development backlog, unless it has direct impact to the critical service of customers.\n",
      "-------\n",
      "QUESTION: How often are penetration tests carried out and by whom? Pentest, \n",
      "ANSWER: Penetration testing is performed at every major release and at least once per year. Penetration testing is performed by NVISO, but we reserve the right to switch to another trusted and accredited company offering similar services.\n",
      "-------\n",
      "QUESTION: Do you conduct local operating system-layer vulnerability scans regularly as prescribed by industry best practices?, \n",
      "ANSWER:\n",
      "-------\n",
      "QUESTION: Expected evidence: Vulnerability & Patch Management Process overview, Patching Performance Reporting.Â , \n",
      "ANSWER:\n",
      "-------\n",
      "\n",
      " ## Answer:\n",
      "\n",
      "Yes, Metamaze performs regular penetration tests to evaluate the security controls and identify potential vulnerabilities. These tests are performed at every major release and at least once per year.\n"
     ]
    }
   ],
   "source": [
    "q(\"\"\"Does the provider perform regular vulnerability assessments / penetration tests to determine security gaps? \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf70a1-b150-47bc-af58-9de50d0cd07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b4c5e-0b37-4134-b208-92d8fdfa0a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0eb889-f469-49f2-b542-0741f46db1b3",
   "metadata": {},
   "source": [
    "## Make Apify crawl vlierck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67144b43-6ec8-46ce-83d6-cc1881ca4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import ApifyWrapper\n",
    "import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API key\"\n",
    "os.environ[\"APIFY_API_TOKEN\"] = \"apify_api_X4ssRRKfPInbJv5HV24mBkwpkhTs084rrM3o\"\n",
    "\n",
    "apify = ApifyWrapper()\n",
    "# Call the Actor to obtain text from the crawled webpages\n",
    "loader = apify.call_actor(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    run_input={\n",
    "        \"startUrls\": [{\"url\": \"https://www.vlerick.com/en/\"}]\n",
    "    },\n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b51e1d-d731-4d93-aff5-4a191740e987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba6dee-4871-42f4-a22e-371437e57d41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
